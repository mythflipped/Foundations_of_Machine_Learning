{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16d4d76a",
      "metadata": {
        "id": "16d4d76a"
      },
      "source": [
        "# General Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7172839",
      "metadata": {
        "id": "b7172839"
      },
      "source": [
        "Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the [Code of Conduct](https://kth.instructure.com/courses/32018/pages/code-of-conduct). In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions.\n",
        "\n",
        "If you need assistance with the exercise, you are encouraged to post a question to the appropriate [Discussion Topic](https://kth.instructure.com/courses/32018/discussion_topics) or sign up for a help session.\n",
        "\n",
        "<br>\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and delete the `raise NotImplementedError()` once you have implemented the solution\n",
        "\n",
        "<br>\n",
        "\n",
        "You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all` then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows\n",
        "\n",
        "<br>\n",
        "\n",
        "`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`\n",
        "\n",
        "<br>\n",
        "\n",
        "where you replace `??` with the correct exercise number. If you are working alone you do not need to include a partner name. Correctly naming the file and including your name (and your partner's) below is worth **1 point** - if you fail to correctly name the file or include your partner's name, *you will lose this point*.\n",
        "\n",
        "<br>\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ffd7e3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4e10dd9ff9316230e586bc2ddf7c902a",
          "grade": false,
          "grade_id": "cell-d2aae5414ee22e91",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "c1ffd7e3"
      },
      "source": [
        "# Name (1 pts)\n",
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly): <font color=\"red\"> (1 Point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b14431b",
      "metadata": {
        "id": "7b14431b"
      },
      "source": [
        "\n",
        "### Student: LAST, FIRST\n",
        "\n",
        "### Partner: LAST, FIRST"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf36b9f",
      "metadata": {
        "id": "fdf36b9f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74291beb",
      "metadata": {
        "id": "74291beb"
      },
      "source": [
        "# Exercise 7: Machine learning and probability (26 points)\n",
        "\n",
        "In this exercise, you will apply a vartiety density-estimation techniques to a number of problems in one or more dimensions. The excercise contains two parts:\n",
        "1. One-dimensional problems with MLE, MAP, and KDE\n",
        "2. A two-dimensional problem with GMMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e0d8ce",
      "metadata": {
        "id": "79e0d8ce"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from scipy.special import loggamma\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "# Fixing the random seed for reproducibility\n",
        "RANDOM_SEED=4321\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bearing-estate",
      "metadata": {
        "id": "bearing-estate"
      },
      "source": [
        "## 7.1 One-dimensional problems (19.5 points)\n",
        "\n",
        "In this part of the exercise, you will perform density-estimation on three different datasets in one dimension. We will call these datasets dataset A, dataset B, and dataset C. All the data and distributions in this exercise are known to be nonnegative.\n",
        "\n",
        "To analyse the data and to fit densities to it, you will use maximum likelihood, Bayesian methods (maximum a posteriori), and kernel density estimation. The problems include both hands-on coding and theoretical derivations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e8726f8",
      "metadata": {
        "id": "7e8726f8"
      },
      "source": [
        "### Loading the datasets\n",
        "\n",
        "The below code downloads a JSON file with the datasets you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0046a91",
      "metadata": {
        "id": "a0046a91"
      },
      "outputs": [],
      "source": [
        "!curl https://gist.githubusercontent.com/shivammehta25/74b9d6f88462d59599a37a42597fac7c/raw/b0d4faec83395f3715256893da92033d8dffc7d3/assignment_6_data.json --output data.json\n",
        "\n",
        "assert os.path.exists('data.json'), \"Please download the data by running the cell above.\"\n",
        "\n",
        "import json\n",
        "with open('data.json') as data:\n",
        "    data = json.load(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53b53bf7",
      "metadata": {
        "id": "53b53bf7"
      },
      "source": [
        "To get an idea of what the data looks like, first load the three different datasets into memory using the command `np.load`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e71fe41",
      "metadata": {
        "id": "8e71fe41"
      },
      "outputs": [],
      "source": [
        "dataset = data['part_1']\n",
        "for ds in dataset:\n",
        "    # Note: Do not make changes to the structure of the datasets;\n",
        "    #       just load the dataset into the keys.\n",
        "    print(\"Dataset: \" + str(ds))\n",
        "    dataset[ds] = np.array(dataset[ds])\n",
        "    print(\"Number of points: \" + str(len(dataset[ds])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48dde2ce",
      "metadata": {
        "id": "48dde2ce"
      },
      "source": [
        "The `dataset` dictionary now contains 3 keys `[A, B, C]`, each of which will return the corresponding dataset `A`, `B` or `C`, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8d3a09",
      "metadata": {
        "id": "5d8d3a09"
      },
      "source": [
        "### Visualising the datasets\n",
        "\n",
        "The first thing to do when dealing with a new dataset is to try to get an idea of what it looks like, and what is going on. One-dimensional data is generally quite easy to take in visually. Here's we'll start with a histogram, which is a very common visualisation in one dimension and can be seen as a (somewhat crude) density esttimation method.\n",
        "\n",
        "**7.1.1 Task:**<br/>\n",
        "Plot a histogram for each of the three datasets using the command `axes.hist`, using the parameter `density=True` to normalise them into a probability distribution.\n",
        "For each dataset, play around with the number of bins until you think you have a histogram that you think gives a good picture of what is going on, and then include that plot below. <font color=\"red\">(1 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "Since we know that these data come from nonnegative distributions, the horizontal axis in any of the one-dimensional plots in this exercise should not include negative numbers.\n",
        "\n",
        "_Hint:_<br/>\n",
        "Like many other plots in this part of the exercise, you will be returning to this visualisation later on. It can therefore be useful to store properties such as the number of bins in a variable, so that all histograms in your notebook change in synchrony in case you decided to change these properties later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4979a6d8",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b54b2fb555f4572aa4c7e757449ae27e",
          "grade": true,
          "grade_id": "cell-e24ae2a3c5e701fe",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "4979a6d8"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "# Hint: Use the axes ax1, ax2 and ax3 for plotting\n",
        "ax1.set_title('Dataset A')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d97a110",
      "metadata": {
        "id": "6d97a110"
      },
      "source": [
        "**7.1.2 Question:**\n",
        "Do the datasets seem to come from a discrete or a continuous distribution? Write one or two sentences to motivate your answer. <font color=\"red\">(0.5 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a6c231",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "95680e1aa6160fa443458265b6e4673b",
          "grade": true,
          "grade_id": "cell-1154ac372ea95e9c",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "12a6c231"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e8a0daf",
      "metadata": {
        "id": "7e8a0daf"
      },
      "source": [
        "### Kernel density estimation\n",
        "\n",
        "Histograms are not good at capturing the actual shape of a distribution. To get a better idea of what the actual distributions look like, we can apply kernel density estimation instead. Let’s do that here.\n",
        "\n",
        "**7.1.3 Task:**<br/>\n",
        "Use `sklearn.neighbors.KernelDensity`, which has been imported for you, to estimate the density of each dataset. Let the kernels be `gaussian` (also known as the _squared exponential kernel_) and try to tune the bandwidth separately for each dataset to a value that you think gives the most accurate impression of the shape of the underlying distribution.\n",
        "Do this by plotting and re-plotting each estimated density for a wide range of different bandwidth values, and home in on what you think looks the most accurate. Show the final plots, with your selected bandwidths, below. <font color=\"red\">(1.5 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "Note that “giving the most accurate impression of the shape of the underlying distribution” is _not_ the same thing as “being as similar as possible to the histogram”. Histograms can be misleading.\n",
        "\n",
        "_Hint:_<br/>\n",
        "To help you get started, we have provided example code for how to use `sklearn.neighbors.KernelDensity`.\n",
        "Since `sklearn.neighbors.KernelDensity` expects input of shape `(n_samples, n_features)`, the example code shows you how to add an extra dimension in NumPy using `data[:, np.newaxis]`. If you want additional information regarding the syntax for interacting with `sklearn.neighbors.KernelDensity`, the following tutorial might be useful: https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html\n",
        "\n",
        "_Hint:_<br/>\n",
        "Like many other plots in this part of the exercise, you will be returning to this visualisation later on. It can therefore be useful to store important properties (such as the bandwidths, the $x$-values where the KDE is evaluated for each plot, and the values of the KDE pdf at these values) in dedicated variables, so that all KDEs in your notebook change in synchrony in case you decide to change these properties later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326ec78f",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3c86971e983c87a5f334bb8d65b8b3aa",
          "grade": true,
          "grade_id": "cell-ab25b5b66271546d",
          "locked": false,
          "points": 1.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "326ec78f"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "ax1.set_title('Dataset A - KDE')\n",
        "# Example code for making a KDE plot of dataset A (please overwrite the values):\n",
        "bandwidth_a = 0 # KDE bandwidth\n",
        "x_max_a = 0 # Biggest value shown on the horizontal axis\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "kde = KernelDensity(bandwidth=bandwidth_a, kernel='gaussian')\n",
        "kde.fit(dataset['A'][:, None])\n",
        "x_a = np.linspace(0, x_max_a, 100)[:, None]\n",
        "logprob = kde.score_samples(x_a) # score_samples returns the log of the probability density\n",
        "kde_pdf_a = np.exp(logprob)\n",
        "ax1.plot(x_a, kde_pdf_a)\n",
        "\n",
        "# Now you do the same for datasets B and C!\n",
        "\n",
        "ax2.set_title('Dataset B - KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C - KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print the bandwidths you chose for the different datasets here\n",
        "print(\"Bandwidth selected for dataset A: \" + str(bandwidth_a))\n",
        "print(\"Bandwidth selected for dataset B: \" + str(bandwidth_b))\n",
        "print(\"Bandwidth selected for dataset C: \" + str(bandwidth_c))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5708c98",
      "metadata": {
        "id": "e5708c98"
      },
      "source": [
        "**7.1.4 Task:**<br/>\n",
        "To illustrate the effect of small and wide bandwidths, repeat your previous plots for all three datasets, but with the KDE bandwidth set to $\\frac{1}{5}$ and $5$ times the value you chose for each dataset. Superimpose the resulting estimated densities on top of the ones in the three plots above, so that the densities are easy to compare.\n",
        "\n",
        "Put another way, your task is to make the KDE bandwith artificially narrow for each dataset, and plot the KDE that results from that on top of the corresponding density plot from the previous cell. Then make another set of three plots, same as the previous ones except that you use an artificially wide bandwidth instead of an artificially narrow one. <font color=\"red\">(0.5 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "This should be straightforward if you saved the necessary bandwidths, $x$-values, and KDE pdf values for the previous plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50926283",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7d76617d9aca30e33e86079b179e12d6",
          "grade": true,
          "grade_id": "cell-1f7bd6015fd48358",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "50926283"
      },
      "outputs": [],
      "source": [
        "# Repeat the previous plots, but using 1/5 times your chosen bandwidth, overlaid on your original KDE\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "ax1.set_title('Dataset A - Narrow KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B - Narrow KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C - Narrow KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Then repeat the previous plots again, but using 5 times your chosen bandwidth overlaid on your original KDE\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "ax1.set_title('Dataset A - Wide KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B - Wide KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C - Wide KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba4faf5",
      "metadata": {
        "id": "6ba4faf5"
      },
      "source": [
        "**7.1.5 Question:**\n",
        "Comment on how the KDE behaves when the bandwidth is too small, and when the bandwidth is too large. What changes do you see? <font color=\"red\">(0.5 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0414c64",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1d0572e033bef07cdfb10de7fe1f49e2",
          "grade": true,
          "grade_id": "cell-7f28908274829075",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "f0414c64"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fe3a90",
      "metadata": {
        "id": "86fe3a90"
      },
      "source": [
        "There are several simple rules of thumb that can be used to set the kernel bandwidths automatically. However, all of them rely on quite specific assumptions, and often give misleading results when those assumptions are violated. They should therefore be used with caution. (A better, but more complicated approach is to use cross-validation.)\n",
        "\n",
        "The simplest bandwidth-estimation formula based on mathematical principles is perhaps to choose the bandwidth $h$ as $$\\widehat{h}=1.06\\widehat{\\sigma}n^{-\\frac{1}{5}}$$ where n is the number of datapoints and $\\widehat{\\sigma}$ is the standard deviation of the data. This rule is optimal in a certain mathematically sense if the underlying distribution is a 1D Gaussian. Of course, that’s not always helpful, since if the data was Gaussian – and we knew that – we would not be using KDE to begin with. Nonetheless, the rule of thumb offers a place to start.\n",
        "\n",
        "**7.1.6 Task:**<br/>\n",
        "Apply the above rule of thumb to the three datasets. Overlay the resulting density estimates on top of the original density estimates for your selected bandwidths `bandwidth_a`, `bandwidth_b`, and `bandwidth_c` earlier. <font color=\"red\">(0.75 Point)</font>\n",
        "\n",
        "**7.1.7 Question:**<br/>\n",
        "Write a comment in the code stating whether or not the shape of the latest density estimates agrees with the shapes of your original density estimates? <font color=\"red\">(0.25 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445d0d12",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6532e383875786cd19458e93f60f1f3f",
          "grade": true,
          "grade_id": "cell-8ff5c1ad7e764fe5",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "445d0d12"
      },
      "outputs": [],
      "source": [
        "# Based on formula\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "ax1.set_title('Dataset A - KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B - KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C - KDE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# How similar are the distribution shapes to the shapes you found on your own?\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85abb5bb",
      "metadata": {
        "id": "85abb5bb"
      },
      "source": [
        "### Maximum likelihood estimation\n",
        "\n",
        "Thus far, we have seen a few things:\n",
        "* The distribution of dataset A is a bit like a blob, with a longer tail towards the right than towards the left, compared to where the peak is. We say that the data (and its distribution) are _skewed to the right_.\n",
        "* Dataset B has a similar distribution to A, except that there are some extra points with much higher values than before.\n",
        "* Dataset C is not as skewed, and rather looks quite flat and doesn't appear to have much of a tail.\n",
        "\n",
        "Now we will take this a bit further, to see what happens if we fit a parametric family to this data using maximum likelihood. In this exercise, you will use the gamma distribution, which is a scalar, continuous-valued, nonnegative distribution with two parameters. Its probability density function is defined by\n",
        "\n",
        "$$\\text{Gamma}(x;\\alpha,\\beta) := \\frac{\\beta^\\alpha}{ \\Gamma(\\alpha)} x^{\\alpha-1} \\exp (-\\beta x)$$\n",
        "\n",
        "for $\\text x\\geq 0$, where $\\alpha>0$ is the _shape parameter_ (which, surprise surprise, affects the overall shape of the disctribution) while $\\beta>0$ is the _rate parameter_ (which does not affect the shape of the distribution at all, but only stretches or compresses it). The _gamma function_ $\\Gamma(\\alpha)$ in the denominator of the normalisation constant is an extension of the factorial. You can read more about it on Wikipedia: https://en.wikipedia.org/wiki/Gamma_function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a6eed0",
      "metadata": {
        "id": "09a6eed0"
      },
      "source": [
        "Let's first implement the probability density function of the gamma distribution based on the formula shown above.\n",
        "\n",
        "**7.1.8 Task:**<br/>\n",
        "Implement the `pdf` function for the `Gamma` class below. <font color=\"red\">(1.5 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "This is much more numerically stable if you do all computations in logarithmic space, and then apply `np.exp` as the very last step when you return the value of the function.\n",
        "\n",
        "_Hint:_<br/>\n",
        "Use `loggamma` already imported from `scipy.special.loggamma` when needed for computing the value of $\\ln \\Gamma \\left(x\\right)$.\n",
        "\n",
        "_Note:_<br/>\n",
        "If your implementation is correct, you might still get a harmless warning `RuntimeWarning: divide by zero encountered in log` whenever you evaluate the `pdf`function on $x=0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0942fe7",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "314158209e92f307e331992d8f504b2d",
          "grade": false,
          "grade_id": "cell-2d41706d428eee17",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c0942fe7"
      },
      "outputs": [],
      "source": [
        "class Gamma:\n",
        "\n",
        "    def __init__(self, alpha, beta):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def pdf(self, x):\n",
        "        r\"\"\"\n",
        "        Defines the probability distribution function of the gamma distribution\n",
        "        Remember Probability density function's value at some specific point does not give you probability;\n",
        "        It is a measure of how dense the distribution is around that value.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): input data points\n",
        "        Returns:\n",
        "            pdf (np.ndarray): density of the input data points\n",
        "        \"\"\"\n",
        "        if not isinstance(x, np.ndarray):\n",
        "            x = np.array(x)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "979a6bce",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12d5fb94ee1e3beb1f34c7b4829d4b6c",
          "grade": true,
          "grade_id": "cell-9be70d8f2eac1c49",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "979a6bce"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(Gamma(2, 0.2).pdf([1, 4, 5]), np.array([0.03274923, 0.07189263, 0.07357589])).all(), \"Look at the formula again, make sure you parameterised the gamma distribution correctly\"\n",
        "assert np.isclose(Gamma(3, 1).pdf([1, 4, 5]), np.array([0.18393972, 0.14652511, 0.08422434])).all(), \"Look at the formula again, make sure you parameterised the gamma distribution correctly\"\n",
        "assert np.isclose(Gamma(500, 100).pdf([4, 5, 6]), np.array([2.10234691e-05, 1.78382679e+00, 2.15449886e-04])).all(), \"Make sure you perform all computations in logarithmic space until the very end\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c567e7e",
      "metadata": {
        "id": "9c567e7e"
      },
      "source": [
        "For the rest of the exercise, we will keep the shape parameter fixed at $\\alpha=5$ and only consider estimating $\\beta$.\n",
        "\n",
        "**7.1.9 Task:**<br/>\n",
        "To get an idea of how this density behaves, make a plot of this distribution for a number of different $\\beta$-values, say 1, 2, and 3. Use the `pdf` function that you defined above for the plots. <font color=\"red\">(0.5 Point)</font>\n",
        "\n",
        "If you like, you can play around with the shape parameter $\\alpha$ to see what it does, but don't forget to set it back to 5 when you are done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6dcf4c0",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "153e863ca929440a471c242c02afaf9e",
          "grade": true,
          "grade_id": "cell-c8953282499683c7",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "a6dcf4c0"
      },
      "outputs": [],
      "source": [
        "alpha = 5\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4604cd9",
      "metadata": {
        "id": "f4604cd9"
      },
      "source": [
        "**7.1.10 Question:**<br/>\n",
        "Looking at the shape, do you think this distribution is a good fit for dataset A, B, and C, respectively? Give one or two sentences of motivation. <font color=\"red\">(0.5 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3a335c",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "401a69787f986c66569a6b3d432a9486",
          "grade": true,
          "grade_id": "cell-fc44cdbd35ee532b",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5e3a335c"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8e44d0",
      "metadata": {
        "id": "af8e44d0"
      },
      "source": [
        "Regardless of your answer(s) to the previous question, we’re going to plow ahead, just see what happens when we apply maximum-likelihood parameter estimation to fit a gamma distribution to the three datasets. To do this, you will first derive a general formula for estimating $\\beta$ from observations.\n",
        "\n",
        "**7.1.11 Task:**<br/>\n",
        "Let the data be $\\mathcal{X}=\\{x_i\\}_{i=1}^n$. Derive the log likelihood of the data as a function of $\\beta$. Show the derivation and the final formula below. <font color=\"red\">(1.5 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e90829e",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f45c6f116bc204306e3a3f01080f9933",
          "grade": true,
          "grade_id": "cell-eaa8da6fc1590c4a",
          "locked": false,
          "points": 1.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5e90829e"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4440b0",
      "metadata": {
        "id": "dd4440b0"
      },
      "source": [
        "**7.1.12 Task:**<br/>\n",
        "Plot the log-likelihood function you just derived for the case of dataset A over the range $\\beta\\in[0.5, 15]$. <font color=\"red\">(0.5 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0a11a2",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "53e0f087ec51f7a40c4beec3a314f48b",
          "grade": true,
          "grade_id": "cell-c6f36b45ace1283c",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8a0a11a2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.set_title('Log likelihood as a function of beta (dataset A)')\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea3b0b1",
      "metadata": {
        "id": "4ea3b0b1"
      },
      "source": [
        "**7.1.13 Task:**<br/>\n",
        "Next, derive a formula for the maximum likelihood estimate $\\widehat{\\beta}_{\\mathrm{ML}}$ as a function of the data $\\mathcal{X}$ and the shape parameter $\\alpha$. Show your work and the final formula below. <font color=\"red\">(1.5 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27ef5de",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f9b60fc7abae707042ec7e9168e3940d",
          "grade": true,
          "grade_id": "cell-6c4090e2169e5afa",
          "locked": false,
          "points": 1.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "d27ef5de"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792a9844",
      "metadata": {
        "id": "792a9844"
      },
      "source": [
        "**7.1.14 Task:**<br/>\n",
        "Write a small function that computes the maximum-likelihood estimate, based on the previously defined variable `alpha` and on the input `dataset`. <font color=\"red\">(0.25 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b1b8a8",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "decbd67bbcbc19e3059623a30e7b9d06",
          "grade": false,
          "grade_id": "cell-2d45d9468d944496",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "81b1b8a8"
      },
      "outputs": [],
      "source": [
        "def beta_hat_ml(dataset):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78237623",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1e0bea1d074512f4d8624b20c59d66b1",
          "grade": true,
          "grade_id": "cell-40290802768bfae1",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "78237623"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(beta_hat_ml([1, 2, 3, 4, 5]), 1.66666, atol=1e-4), \"Make sure that alpha is set to 5\"\n",
        "assert np.isclose(beta_hat_ml([6.5, 7.5, 8.5, 9.5, 10.5]), 0.5882, atol=1e-4), \"Check the formula and derivation again\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a0d1b7",
      "metadata": {
        "id": "61a0d1b7"
      },
      "source": [
        "**7.1.15 Task:**<br/>\n",
        "Apply the MLE function that you implemented to datasets A, B, and C. Plot the histograms and your prefered kernel density estimates from before, but with the respective maximum-likelihood estimated probability density function overlaid as a line plot. That is, each (sub)plot should contain three things: a histogram (like you plotted earlier in the exercise), a KDE (also from earlier in the exercise), and your new, ML-estimated gamma density function. <font color=\"red\">(1 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "This should be straightforward if you saved the necessary histogram parameters, KDE bandwidths, $x$-values, and KDE pdf values from the previous plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1068c5aa",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1a9e52f81c69e4a4336dfd5c56ae5b53",
          "grade": true,
          "grade_id": "cell-4b40f1dacb9c663b",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1068c5aa"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "# Hint: Use the axes ax1, ax2 and ax3 for plotting\n",
        "ax1.set_title('Dataset A – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c445cd0",
      "metadata": {
        "id": "1c445cd0"
      },
      "source": [
        "Let's compare datasets A and B by computing the following.\n",
        "\n",
        "**7.1.16 Task:**<br/>\n",
        "You might not have noticed this, but many of the datapoints (i.e., the $x$ values) in dataset B are also in dataset A. Compute how many of the datapoints in B that are not in A. <font color=\"red\">(0.5 Point)</font>\n",
        "\n",
        "**7.1.17 Task:**<br/>\n",
        "How much do the values of $\\widehat{\\beta}_{\\mathrm{ML}}$ differ between the two datasets? Compute and print the two beta values and their difference. <font color=\"red\">(0.5 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "short-relaxation",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "75a52518817b51bb4cfa8e7d13553a01",
          "grade": true,
          "grade_id": "cell-eff143c1bbd20d26",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "short-relaxation"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparative-letter",
      "metadata": {
        "id": "comparative-letter"
      },
      "source": [
        "Interesting that so few datapoints can make such a big difference, huh? A lot of this can be explained as an effect of misspecification, as discussed in the module."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b206b97f",
      "metadata": {
        "id": "b206b97f"
      },
      "source": [
        "### Maximum a posteriori\n",
        "\n",
        "Next up, we'll compare the maximum-likelihood estimate to a Bayesian approach. To do this, we need a prior distribution that represents our prior belief about how probable different values of $\\boldsymbol\\beta$ are. (Note that $\\boldsymbol\\beta$ now is a random variable.) We will use another gamma distribution for the prior density of $\\boldsymbol\\beta$. This prior will have parameters $a>0$ and $b>0$:\n",
        "\n",
        "$$ \\text{Gamma}(\\beta;a,b) = \\frac{b^a \\beta^{a-1} \\exp \\left(-b\\beta \\right)}{\\Gamma(a)} $$\n",
        "\n",
        "**7.1.18 Task:**<br/>\n",
        "Plot this density over the range $[0,12]$ using $a=5$ and $b=1$, $a=50$ and $b=10$, and $a=500$ and $b=100$. <font color=\"red\">(0.5 Point)</font>\n",
        "\n",
        "_Note:_<br/>\n",
        "If your implementation of the `pdf` function does not use logarithmic space correctly for numerical safety, the final plot will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8055c38",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "21100802cecdfd6c4ccc5a9bfbd7d267",
          "grade": true,
          "grade_id": "cell-e9920ad262066b5b",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c8055c38"
      },
      "outputs": [],
      "source": [
        "a_b_list = [(5, 1), (50, 10), (500, 100)]\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breeding-technology",
      "metadata": {
        "id": "breeding-technology"
      },
      "source": [
        "You might notice that the distribution stays in approximately the same place, but concentrates on a narrower interval as the values of the arguments increase."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284da1ba",
      "metadata": {
        "id": "284da1ba"
      },
      "source": [
        "**7.1.19 Task:**<br/>\n",
        "Using the formula for the gamma prior, derive a closed-form expression for the maximum a posteriori estimate $\\widehat{\\beta}_{\\mathrm{MAP}}$. Show the steps of the derivation and the final formula below. <font color=\"red\">(3 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "The answer will be a function of the observations $x_i$ in the dataset $\\mathcal{X}$ and also $a$ and $b$. Work in the log space, like for the MLE."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf21fb1c",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3edff506b36b20ebcafbe9847a096a75",
          "grade": true,
          "grade_id": "cell-6fddbd739880f20c",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bf21fb1c"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "minus-tyler",
      "metadata": {
        "id": "minus-tyler"
      },
      "source": [
        "**7.1.20 Task:**<br/>\n",
        "Write a small function that computes the MAP parameter estimate of $\\beta$, based on the previously defined variable `alpha` and on the inputs `dataset`, `a`, and `b`. <font color=\"red\">(0.25 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc695372",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e765f3358c0c2f28b8ba0631333a1c78",
          "grade": false,
          "grade_id": "cell-349390e3644b72de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fc695372"
      },
      "outputs": [],
      "source": [
        "def beta_hat_map(dataset, a, b):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "widespread-volleyball",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5e4a69e26df4b852ee7f5b8c12f560cf",
          "grade": true,
          "grade_id": "cell-b8520302fa8b280c",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "widespread-volleyball"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(beta_hat_map(np.array([1, 2, 3]), 5, 2), 2.375, atol=1e-4)\n",
        "assert np.isclose(beta_hat_map(np.array([7, 8, 10]), 2, 10), 0.45714, atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sustainable-backup",
      "metadata": {
        "id": "sustainable-backup"
      },
      "source": [
        "**7.1.21 Task:**<br/>\n",
        "Apply the MAP function that you just implemented to datasets A, B, and C for the prior parameters $a=5$, $b=1$. Plot the histograms and the ML-estimated density from before, but with the respective maximum a-posteriori estimated probability density function overlaid as a line plot. That is, each plot should contain a histogram and two gamma densities: one from MLE and the other based on the MAP parameter estimate. <font color=\"red\">(0.25 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af164816",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6bf7c49fa969dcb4e0355d44525851b3",
          "grade": true,
          "grade_id": "cell-7f292d2cf9db71ab",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "af164816"
      },
      "outputs": [],
      "source": [
        "a, b = 5, 1\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "# Hint: Use the axes ax1, ax2 and ax3 for plotting\n",
        "ax1.set_title('Dataset A – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "minus-timothy",
      "metadata": {
        "id": "minus-timothy"
      },
      "source": [
        "**7.1.22 Task:**<br/>\n",
        "Repeat the plots in the previous cell, but using $a=500$, $b=100$ for the prior instead. As you saw in a previous plot you made, this gives a very narrow prior centered on $\\beta\\approx5$. <font color=\"red\">(0.5 Point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "republican-spectrum",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8f48ec7069223ddf327a684fdb389f3",
          "grade": true,
          "grade_id": "cell-4f4344e6c26593a5",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "republican-spectrum"
      },
      "outputs": [],
      "source": [
        "a, b = 500, 100\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
        "\n",
        "# Hint: Use the axes ax1, ax2 and ax3 for plotting\n",
        "ax1.set_title('Dataset A – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax2.set_title('Dataset B – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "ax3.set_title('Dataset C – MLE')\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amino-camera",
      "metadata": {
        "id": "amino-camera"
      },
      "source": [
        "**7.1.23 Question:**<br/>\n",
        "Comment on the effect of a wide versus a narrow prior on $\\boldsymbol\\beta$. In what case does $\\widehat{\\beta}_{\\mathrm{MAP}}$ deviate notably from $\\widehat{\\beta}_{\\mathrm{ML}}$? Why are the estimates different in this case? When could an ability to constrain the parameter estimates like this be useful? <font color=\"red\">(1 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interesting-penny",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3db840c6fee8d0ec3191d04b9fb7f379",
          "grade": true,
          "grade_id": "cell-d9a8666bb54cc0c5",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "interesting-penny"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "private-malpractice",
      "metadata": {
        "id": "private-malpractice"
      },
      "source": [
        "## 7.2 A two-dimensional problem (5.5 points)\n",
        "\n",
        "In this part of the exercise, we will work on a small, two-dimensional toy dataset. You will implement the formulas for EM parameter estimation with a GMM in two dimensions, and then use this to train (fit) a few GMMs to the dataset. When you do this, you will run into some... interesting issues, and learn a very useful trick for making mixture models work well in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "horizontal-store",
      "metadata": {
        "id": "horizontal-store"
      },
      "source": [
        "### Loading and visualising the data\n",
        "\n",
        "We begin by loading the dataset. (A test set is included, but not currently used.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a4bba3",
      "metadata": {
        "id": "81a4bba3"
      },
      "outputs": [],
      "source": [
        "dataset = data['part_2']\n",
        "\n",
        "X_train = np.array(dataset['X_train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "portuguese-robert",
      "metadata": {
        "id": "portuguese-robert"
      },
      "source": [
        "To get an idea of what this data is like, we print the shape of the training and test sets and visualise the data using a 2D scatterplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1b7a553",
      "metadata": {
        "id": "b1b7a553"
      },
      "outputs": [],
      "source": [
        "# Printing the shapes\n",
        "print(f\"Dataset shape:\\t {X_train.shape}\")\n",
        "\n",
        "# Visualizing the data\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1])\n",
        "ax.set_title('Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "therapeutic-planner",
      "metadata": {
        "id": "therapeutic-planner"
      },
      "source": [
        "You will now implement the update equations for an _axis-aligned_ GMM. “Axis-aligned” means that the covariance matrix of each component Gaussian is diagonal (i.e., the two dimensions are uncorrelated), which simplifies the mathematics. (Recall that, for a diagonal covariance matrix, parameter estimates can be computed independently for each dimension.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820161c1",
      "metadata": {
        "id": "820161c1"
      },
      "outputs": [],
      "source": [
        "# These are helper functions. Please don't make any changes to these.\n",
        "\n",
        "def make_array(x):\n",
        "    if not isinstance(x, np.ndarray):\n",
        "            x = np.array(x)\n",
        "    return x\n",
        "\n",
        "def generate_data_space(grid_region_min=-8, grid_region_max=8):\n",
        "    x = np.linspace(grid_region_min, grid_region_max, 500)\n",
        "    y = np.linspace(grid_region_min, grid_region_max, 500)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    pos = np.dstack((xx, yy))\n",
        "    return x, y, xx, yy, pos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ready-italian",
      "metadata": {
        "id": "ready-italian"
      },
      "source": [
        "### The E-step\n",
        "\n",
        "The first thing you will do is to implement the log probability density function $\\ln \\mathcal{N}_{\\boldsymbol x}(x;\\mu, \\mathrm{diag}(\\sigma)^2)$ of a multivariate Gaussian distribution with mean $\\mu$ and a diagonal covariance matrix $\\mathrm{diag}(\\sigma)^2$. This function has two inputs:\n",
        "* One or more observations `x` of shape `(n_obs, n_dim)`, where `n_obs > 0` is the number of observations $x_i$ and `n_dim > 0` is the number of elements in each vector $x_i$. This is the same format as that used by `X_train` above.\n",
        "* Model parameters `mu` and `sigma`, both of shape `(n_dim)`, which contain a mean $\\mu_d$ and a standard deviation $\\sigma_d$ for each dimension.\n",
        "\n",
        "The function should return one output:\n",
        "* `log_gauss_pdf` of shape `(n_obs)`, containing the logarithm of the Gaussian pdf evaluated at each observation in `x`.\n",
        "\n",
        "\n",
        "**7.2.1 Task:**<br/>\n",
        "Implement the function `log_gauss_pdf` according to the above specification. <font color=\"red\">(1 Point)</font>\n",
        "\n",
        "_Remark:_<br/>\n",
        "Please make your code work for any dimensionality of the $x$-vectors, even though we will have `n_dim=2` for the data considered later in this part.\n",
        "\n",
        "_Hint:_<br/>\n",
        "For a diagonal covariance matrix, as here, the contribution to the log-probability due to each dimension $d$ can be compluted independently, and then summed. More explicitly, the probability density function of a Gaussian with a diagonal covariance matrix can be written\n",
        "\n",
        "$$ \\mathcal{N}_{\\boldsymbol x}(x;\\mu, \\mathrm{diag}(\\sigma)^2) =  (\\sqrt{2\\pi})^{-D} \\prod_{d=1}^D \\dfrac{1}{\\sigma_d} \\exp \\left( -\\dfrac{(x_d - \\mu_d)^2}{2\\sigma_d^2} \\right) \\text{,}$$\n",
        "\n",
        "where $\\sigma_d^2$ is the $d$th element of the diagonal covariance matrix. Also, keep in mind that the standard deviation $\\sigma$ is not the same thing as the variance $\\sigma^2$. We have added a few test cases below to guide you. If any bugs remain, there is another opportunity to catch them when you run the complete EM-algorithm to train GMMs later on. If you want to read more, additional information about Gaussians with diagonal covariance matrices can be found here: https://markusthill.github.io/gaussian-distribution-with-a-diagonal-covariance-matrix/\n",
        "\n",
        "_Important:_<br/>\n",
        "Do all computations in the logarithmic domain! If you first compute $\\mathcal{N}_{\\boldsymbol x}(x;\\mu, \\mathrm{diag}(\\sigma)^2)$ and then take the logarithm, you will lose a lot of numerical precision and training _will_ fail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coordinated-hollow",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "171291e9001e5028d939ed67c7e3c003",
          "grade": false,
          "grade_id": "cell-95d3444370d88b11",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "coordinated-hollow"
      },
      "outputs": [],
      "source": [
        "def log_gauss_pdf(x, mu, sigma):\n",
        "    x, mu, sigma = make_array(x), make_array(mu), make_array(sigma)\n",
        "\n",
        "    assert len(x.shape) > 1, f\"The input should be in shape of (n_obs, n_dim). {x.shape} passed.\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vanilla-screening",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fcc2732c1d16b84a28be66551477e86c",
          "grade": true,
          "grade_id": "cell-79e51a96d3701828",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vanilla-screening"
      },
      "outputs": [],
      "source": [
        "assert np.isclose(np.exp(log_gauss_pdf(x=[[0.2, 1.2]], mu=[0, 1], sigma=[1, 1])), np.array([0.1529]), atol=1e-4), \"The value is incorrect check the formula again\"\n",
        "assert np.isclose(log_gauss_pdf(x=[[0.2, 1.2], [-0.2, 0.8]], mu=[0, 1], sigma=[1, 1]),  np.array([-1.8778, -1.8778]), atol=1e-4).all(), \"The value is incorrect check the formula again\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "disturbed-cornwall",
      "metadata": {
        "id": "disturbed-cornwall"
      },
      "source": [
        "Once you have access to the density function $\\mathcal{N}_{\\boldsymbol x}(x;\\mu, \\mathrm{diag}(\\sigma^2))$ of a single multivariate, axis-aligned Gaussian (or, as here, its logarithm), it's quite easy to implement the E-step of the EM-algorithm. All that needs to be done is to compute the probability of each datapoint having come from the each component, under the current model $\\theta=\\{\\pi_j,\\mu^{(j)},\\sigma^{(j)}\\}_{j=1}^m$.\n",
        "\n",
        "The E-step of the EM algorithm takes data and model parameters as input and computes the responsibilities. It has two inputs:\n",
        "* The training-data observations `x` of shape `(n_obs, n_dim)`.\n",
        "* The parameters $\\theta$ of the GMM, comprising `weights` of shape `(n_components)` and `mus` and `sigmas` of shape `(n_components, n_dim)`, i.e., one $\\mu$-vector and one $\\sigma$-vector for each component $j$.\n",
        "\n",
        "The E-step returns two outputs:\n",
        "* `gammas` of shape `(n_obs, n_components)`, containing the responsibilities $\\gamma_{ij}(\\theta):=P_{\\pmb z|\\pmb x}(j|x_i;\\theta)$, i.e., the conditional probability that component $j$ was responsible for observation $x_i$, according to the current model parameters.\n",
        "* `log_likelihood` of shape `(n_obs)`, which contains the. This output is not strictly necessary, but only included for convenience, since it is easy to compute the log likelihood of the current model as a side product when computing the responsibilities.\n",
        "\n",
        "To make things easier, _we have already implemented the E-step for you._ You do not need to do that. You should, however, study the code so that you understand what is going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floppy-forest",
      "metadata": {
        "id": "floppy-forest"
      },
      "outputs": [],
      "source": [
        "# The e_step function has already implemented, for your convenience.\n",
        "\n",
        "def e_step(x, weights, mus, sigmas):\n",
        "    x, weights, mus, sigmas = make_array(x), make_array(weights), make_array(mus), make_array(sigmas)\n",
        "\n",
        "    n_components = weights.shape[0]\n",
        "    log_responsibilities = np.zeros((x.shape[0], n_components))\n",
        "    for j in range(n_components):\n",
        "        log_responsibilities[:, j] = np.log(weights[j]) + log_gauss_pdf(x, mus[j], sigmas[j])\n",
        "\n",
        "    log_likelihood = logsumexp(log_responsibilities, axis=1)\n",
        "    log_responsibilities = log_responsibilities - log_likelihood[:, np.newaxis]\n",
        "\n",
        "    return np.exp(log_responsibilities), log_likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "essential-mailman",
      "metadata": {
        "id": "essential-mailman"
      },
      "source": [
        "**7.2.2 Question:**<br/>\n",
        "Why does the code use a special SciPy function to compute the following line\n",
        "```python\n",
        "log_likelihood = logsumexp(log_responsibilities, axis=1)\n",
        "```\n",
        "and not the more straightforward solution\n",
        "```python\n",
        "log_likelihood = np.log(np.exp(log_responsibilities).sum(axis=1))\n",
        "```\n",
        "? Give one to three sentences of motivation.\n",
        "\n",
        "_Hint:_<br/>\n",
        "There is a functional difference; it's not merely that the former line is shorter to type. <font color=\"red\">(1 Point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moved-proceeding",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "aefb3462d1cbd069633bd8d0dac9829c",
          "grade": true,
          "grade_id": "cell-1260a17e4e7cac8e",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "moved-proceeding"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hourly-tracy",
      "metadata": {
        "id": "hourly-tracy"
      },
      "source": [
        "### The M-step\n",
        "\n",
        "The M-step of the EM algorithm uses the responsibilities (and the data) to compute updated parameter estimates $\\theta^{\\mathrm{new}}$. It has two inputs:\n",
        "* The training-data observations `x` of shape `(n_obs, n_dim)`.\n",
        "* The responsibilities `gammas`, of shape `(n_obs, n_components)`, representing the probabilities $\\gamma_{ij}(\\theta)$ that each observation $x_i$ was generated by the component $\\mathcal{N}(x;\\mu^{(j)}, \\mathrm{diag}(\\sigma^{(j)})^2)$.\n",
        "\n",
        "The M-step should return three outputs, as a single tuple:\n",
        "* `(weights, mus, sigmas)` containing updated parameter estimates $\\widehat\\theta^{\\mathrm{new}}$. Specifically, `weights` has shape `(n_components)` while `mus` and `sigmas` have shape `(n_components, n_dim)`, i.e., one $\\mu$-vector and one $\\sigma$-vector for each component $j$.\n",
        "\n",
        "**7.2.3 Task:**<br/>\n",
        "Implement the function `m_step` according to the above specification. The code should work for `n_components > 0`, including `ncomponents == 1`. <font color=\"red\">(2 Point)</font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "The lecture notes have formulas for the general case, where the covariance matrix $\\Sigma^{(j)}$ is a full matrix. In this exercise, however, we are working with covariance matrices that are restricted to be diagonal, $\\Sigma^{(j)}=\\mathrm{diag}((\\widehat\\sigma^{(j)})^2)$. Since we are working with Gaussian distributions, this means that each dimension is independent of each other dimension. For this reason, one can apply the one-dimensional version of the update formula for the covariance in the lecture notes to compute the variances $(\\widehat\\sigma^{(j)})^2$ for each dimension (each element in the $\\widehat\\sigma^{(j)}$-vector) separately.\n",
        "\n",
        "_Hint:_<br/>\n",
        "If implemented correctly, `weights > 0` and `np.sum(weights) == 1`, the new mean estimates `mus` should always lie within the convex hull of the datapoints, and `sigma > 0`.\n",
        "\n",
        "_Hint:_<br/>\n",
        "We have added a few test cases below to guide you. If any bugs remain, there is another opportunity to catch them when you run the complete EM-algorithm to train GMMs later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metric-grade",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a33aaf9f820adb6ef9ae55b5af19147d",
          "grade": false,
          "grade_id": "cell-a9e0eb28ce37b6e0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "metric-grade"
      },
      "outputs": [],
      "source": [
        "def m_step(x, gammas, **kwargs):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "limiting-evidence",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaafc5bcc0be91eba07e7a2499401716",
          "grade": true,
          "grade_id": "cell-34824d8014c8ff40",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "limiting-evidence"
      },
      "outputs": [],
      "source": [
        "test_ws, test_mus, test_sigmas = m_step(np.array([[0, 1], [1, 0]]), np.array([[1], [2]]))\n",
        "assert np.isclose(test_ws, np.array([1.5]), atol=1e-4), \"Incorrect implementation. Check the formulas again\"\n",
        "assert np.isclose(test_mus, np.array([[0.6666, 0.3333]]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n",
        "assert np.isclose(test_sigmas, np.array([[0.47140452, 0.47140452]]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n",
        "\n",
        "\n",
        "test_ws, test_mus, test_sigmas = m_step(np.array([[0, 1], [1, 0]]), np.array([[0.5, 0.5], [0.2, 0.8]]))\n",
        "\n",
        "assert np.isclose(test_ws, np.array([0.35, 0.65]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n",
        "assert np.isclose(test_mus, np.array([[0.2857, 0.7142], [0.6153, 0.3846]]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n",
        "assert np.isclose(test_sigmas, np.array([[0.4517, 0.4517], [0.4865, 0.4865]]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "needed-recruitment",
      "metadata": {
        "id": "needed-recruitment"
      },
      "source": [
        "### Initialisation and training loop\n",
        "\n",
        "For initialisation, we'll use a simple recipe to come up with inital parameters $\\theta_0$. Specifically, we just pick `n_components` observations at random from `x` and use these for the means `mus`. The weights are initialised to represent a uniform distribution, while the standard deviations `sigma[:, d]` are proportional to the standard deviation of data in dimension `d`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "internal-personal",
      "metadata": {
        "id": "internal-personal"
      },
      "outputs": [],
      "source": [
        "def init_gmm(x, n_components):\n",
        "\n",
        "    global RANDOM_SEED\n",
        "\n",
        "    n, n_dim = x.shape\n",
        "    np.random.seed(RANDOM_SEED) # resetting the seed\n",
        "\n",
        "    # Init mu from the datapoints\n",
        "    mus_indices = np.random.choice(n, n_components, replace=False)\n",
        "    mus = x[mus_indices]\n",
        "\n",
        "    # Uniform weights\n",
        "    weights = np.full(n_components, 1/n_components)\n",
        "\n",
        "    # Standard deviation from the standard deviation of data scaled by the numer of components\n",
        "    component_std = x.std(axis=0)\n",
        "    sigmas = np.full((n_components, n_dim), component_std / np.sqrt(n_components))\n",
        "\n",
        "    return (weights, mus, sigmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entire-operations",
      "metadata": {
        "id": "entire-operations"
      },
      "source": [
        "Training begins by calling the initialisation routine. The main training loop then performs alternating E-steps and M-steps until a specified number of iterations `n_iter` is reached. Our training loop also includes code to compute and display the evolution of the negative log-likelihood, and code to visualise the data and the GMM during training. You can use the `pause` argument to control the amount of time (in seconds) to pause between each iteration, to adjust the speed of the animation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gothic-berlin",
      "metadata": {
        "id": "gothic-berlin"
      },
      "outputs": [],
      "source": [
        "def train_gmm(x, weights, mus, sigmas, n_iter=30, pause=0.5, **kwargs):\n",
        "\n",
        "    neg_log_likelihood = []\n",
        "    for i in range(n_iter+1):\n",
        "\n",
        "        # 1) Plot GMM\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        plt.suptitle(f\"Iteration: {i}\")\n",
        "\n",
        "        x_axis, y_axis, xx, yy, pos = generate_data_space()\n",
        "\n",
        "        # Compute GMM pdf for heatmap\n",
        "        gmm_pdf = np.zeros(np.shape(pos)[:-1])\n",
        "        for j in range(weights.shape[0]):\n",
        "            weight, mu, sigma = weights[j],  mus[j], sigmas[j]\n",
        "            gmm_pdf = gmm_pdf + np.exp(log_gauss_pdf(pos, mu, sigma)) * weight\n",
        "            #ax[0].contour(xx, yy, z)\n",
        "        ax[0].contourf(xx, yy, np.minimum(gmm_pdf, 0.1), cmap=\"Greys\") # Plot clipped heatmap\n",
        "        ax[0].scatter(x[:, 0], x[:, 1])\n",
        "\n",
        "        for j in range(weights.shape[0]):\n",
        "            weight, mu, sigma = weights[j],  mus[j], sigmas[j]\n",
        "            ax[0].plot([mu[0]-sigma[0], mu[0]+sigma[0]], [mu[1], mu[1]], 'r-')\n",
        "            ax[0].plot([mu[0], mu[0]], [mu[1]-sigma[1], mu[1]+sigma[1]], 'r-')\n",
        "            #component_ellipse = plt.patches.Ellipse(mu, width=2*sigma[0], height=2*sigma[1])\n",
        "            #ax.add_patch(component_ellipse)\n",
        "\n",
        "        ax[0].set_xlabel('x1')\n",
        "        ax[0].set_ylabel('x2')\n",
        "        ax[0].set_title('GMM')\n",
        "\n",
        "        # 2) Run E step\n",
        "        gammas, log_likelihood = e_step(x, weights, mus, sigmas)\n",
        "        mean_neg_log_likelihood = -log_likelihood.mean()\n",
        "\n",
        "        # 3) Plot and print NLL\n",
        "        neg_log_likelihood.append(mean_neg_log_likelihood)\n",
        "        ax[1].plot(neg_log_likelihood)\n",
        "        ax[1].set_title(\"Negative log likelihood\")\n",
        "        ax[1].set_xlabel('iterations')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf()) # Display all plots\n",
        "\n",
        "        if np.isnan(log_likelihood).any():\n",
        "            print(f\"Weights: {weights}\")\n",
        "            print(f\"Mus: {mus}\")\n",
        "            print(f\"Sigma: {sigmas}\")\n",
        "            plt.close(fig)\n",
        "            raise ValueError(f\"Negative log likelihood is NaN in this iteration: {mean_neg_log_likelihood}\")\n",
        "\n",
        "        print(f\"Negative log likelihood per datapoint: {mean_neg_log_likelihood: .5f}\")\n",
        "\n",
        "        if i < n_iter: # Only update parameters if we are not in the last step of the loop\n",
        "            #prev_thetas = weights, mus, sigmas\n",
        "            weights, mus, sigmas = m_step(x, gammas, **kwargs)\n",
        "\n",
        "            time.sleep(pause)\n",
        "\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "    return (weights, mus, sigmas), neg_log_likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rough-albania",
      "metadata": {
        "id": "rough-albania"
      },
      "source": [
        "### Train GMMs on the data\n",
        "\n",
        "The big moment has arrived! Let's try to train a few GMMs to verify that it works and see how GMM training works.\n",
        "\n",
        "**Task:**<br/>\n",
        "Initialise a GMM using `init_gmm`. Then run the command `train_gmm(x, weights, mus, sigmas, n_iter)` to train a GMM on the data for `n_iter=30` iterations. The command returns a tuple representing $\\widehat\\theta_{\\mathrm{ML}}$. Do this for ` 0 < n_components < 6`.\n",
        "\n",
        "During training the command will display an animated visualisation:\n",
        "* The window on the left shows the training curve, tracking how the negative log-likelihood $-\\ln p_{\\mathcal{X}}(\\mathcal{D},\\widehat{\\theta}_u)$ evolves with each training update $u$.\n",
        "* The window on the right shows a scatterplot of the data. Each component centre $\\widehat{\\mu}^{(j)}$ is marked by a cross, whose width and height is equal to the standard deviation $\\widehat{\\sigma}^{(j)}$ in each direction. The overall probability density of the GMM is visualised as a greyscale heatmap.\n",
        "\n",
        "_Hint:_<br/>\n",
        "This is a good opportunity to test your implementation! If everything is correct, there should be no errors/exceptions and training should converge without the log-likelihood ever decreasing. If that is not the case, there is very likely a mistake in `log_gauss_pdf`, `e_step`, or `m_step`, so go back and test your implementation of those functions more.\n",
        "\n",
        "_Important:_<br/>\n",
        "Keep the random seed and the number of iterations at the specified value, to ensure that you get consistent results.\n",
        "\n",
        "_Note:_<br/>\n",
        "This part is not worth any points, since the idea is just to let you test that your implementation works and to learn how it behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bored-structure",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "37d540ac5dac2e75616ae9fa9202df94",
          "grade": true,
          "grade_id": "cell-bb63c09db4b8ee2e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bored-structure"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "finnish-framework",
      "metadata": {
        "id": "finnish-framework"
      },
      "source": [
        "Let's see what happens when training a GMM with $m=10$ components on the data. That should work now, right?\n",
        "\n",
        "_Hint:_<br/>\n",
        "Keep an eye on what happens to the component in the top left quadrant. Move on to the next cell even if you get an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "contemporary-completion",
      "metadata": {
        "id": "contemporary-completion"
      },
      "outputs": [],
      "source": [
        "n_components = 10\n",
        "n_iter = 30\n",
        "\n",
        "initial_theta = init_gmm(X_train, n_components)\n",
        "\n",
        "trained_theta, training_curve = train_gmm(X_train, *initial_theta, n_iter, pause=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chinese-stopping",
      "metadata": {
        "id": "chinese-stopping"
      },
      "source": [
        "That's weird! The log-likelihood seems to be taking off towards infinity, and then training breaks down with some kind of error?\n",
        "\n",
        "This error is not on you, but actually reflects a classic problem that tends to occur when applying GMMs in practice. (If you didn't get an error, something is not according to expectations, but check out if you get an error when you run the next code cell below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "posted-taxation",
      "metadata": {
        "id": "posted-taxation"
      },
      "source": [
        "If you didn't already spot what went wrong above, let's dissect the situation using a more obvious example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rotary-salvation",
      "metadata": {
        "id": "rotary-salvation"
      },
      "outputs": [],
      "source": [
        "initial_weights = np.array([0.995, 0.005])\n",
        "initial_mus = np.array([[0, 0], [-4, 1]]) # The second component sits on top of the outlying pooint\n",
        "initial_sigmas = np.array([[4, 4], [1, 1]]) # The second component is quite narrow\n",
        "\n",
        "initial_theta = initial_weights, initial_mus, initial_sigmas\n",
        "\n",
        "trained_theta, training_curve = train_gmm(X_train, *initial_theta, n_iter=n_iter, pause=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fuzzy-finder",
      "metadata": {
        "id": "fuzzy-finder"
      },
      "source": [
        "**7.2.4 Question:**<br/>\n",
        "What happens to the standard deviation of the second component as training progresses? Why does this cause numerical problems? Give a clear explanation based on the mathematical expression for the Gaussian pdf. <font color=\"red\">(1 Point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "based-thesaurus",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fd13a8b23a8eec576f7b20111171f433",
          "grade": true,
          "grade_id": "cell-a85744f0d13f93a6",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "based-thesaurus"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-syria",
      "metadata": {
        "id": "advanced-syria"
      },
      "source": [
        "### Fixing the problem using variance flooring\n",
        "\n",
        "The above error shows that the likelihood of the data actually is _unbounded_. This is not because the model is infinitely good, which should be intuitive from watching the animation, or from calculating the log-likelihood on validation data. Instead, it's because the model is so flexible that it can find _degenerate solutions_, which are bad edge cases that achieve arbitrarily high likelihoods.\n",
        "\n",
        "Degenerate edge cases exist for _any_ GMM with $m>1$. In fact, every time the standard update formulas settle on a reasonable model and without breaking down, what's really happening is you were rescued from the degeneracies by getting stuck in a local optimum that is not globally optimal. Who knew that local optima sometimes could be a good thing?\n",
        "\n",
        "Degenerate situations pop in lots of different areas of machine learning, both probabilistic and not. I like to think of degeneracies as _specification errors_: we are telling the the machine to optimise a certain number, but we didn't realise that there are solutions to the optimisation problem we formulated that have good values for the objective function, but that are bad in real life. It's a mismatch between what we tell the computer that we want, and what we actually want.\n",
        "\n",
        "To solve the specification error, we need to revise our problem formulation: what kind of solutions that we look for and/or how we find them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moving-dispatch",
      "metadata": {
        "id": "moving-dispatch"
      },
      "source": [
        "Let's return to the situation at hand. This time, the real problem with the GMM is not with the code. Instead, the issue is that the parametric family contains degenerate distributions where the probability density is unbounded. We get close to these degenerate distributions when any $\\sigma$-value in the model gets small.\n",
        "\n",
        "There are many ways to fix this issue, but a widely-used technique called _variance flooring_ is to simply prohibit the $\\sigma$-values from approaching zero small. The way to do so in practice is very simple: At the end of every M-step, check that all elements of `sigma` are greater or equal to a `sigma_min` argument. Any elements of `sigma` that violate this criterion are just set equal to `sigma_min`! `sigma_min**2` thus acts as a “floor” that imposes a lowest-permitted value for the variance.\n",
        "\n",
        "The above might sound like a bit of a hack, but there is a nice theoretical implementation: what we are doing is redefine that we are redefining the parametric family that we are optimising over. Specifically, we remove models within a certain neighbourhood of degenerate parameter values, and only optimise over (now constrained) set of parameters $\\{\\theta:\\sigma_d^{(j)}\\geq\\sigma_{\\mathrm{min}}\\forall j,d\\}$. The act of setting elements $\\sigma_d^{(j)}$ that violate the constraint to $\\sigma_{\\mathrm{min}}$ can be seen as a projection on this feasible set.\n",
        "\n",
        "**8.2.5 Task:**<br/>\n",
        "Re-implement the `m_step` function to incorporate variance flooring. The minimum-permitted standard-deviation value is controlled via the new `sigma_min` argument. <font color=\"red\">(0.5 Point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interesting-printing",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "085db5a770308eec85237177e3af9bc2",
          "grade": false,
          "grade_id": "cell-92a501346c92fe6a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "interesting-printing"
      },
      "outputs": [],
      "source": [
        "def m_step(x, gammas, sigma_min):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "political-bhutan",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "832fcd5318aa103bd2fe3c96f331e350",
          "grade": true,
          "grade_id": "cell-c8dd1b31cade837d",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "political-bhutan"
      },
      "outputs": [],
      "source": [
        "test_ws, test_mus, test_sigmas = m_step(np.array([[0, 1], [1, 1]]), np.array([[0.9, 0.1], [0.1, 0.9]]), 0.3)\n",
        "assert np.isclose(test_ws, np.array([0.5, 0.5]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n",
        "assert np.isclose(test_mus, np.array([[0.1, 1.0], [0.9, 1.0]]), atol=1e-4).all(), \"Incorrect implementation. Check the formulas again\"\n",
        "assert np.isclose(test_sigmas, np.array([[0.3, 0.3], [0.3, 0.3]]), atol=1e-4).all(), f\"Variance flooring not implemented one of the sigmas is less than the floor value of 0.3 passed. sigmas: {test_sigmas}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "southwest-terminal",
      "metadata": {
        "id": "southwest-terminal"
      },
      "source": [
        "Now let's run a few tests to check that this fixes the problem!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adjustable-institution",
      "metadata": {
        "id": "adjustable-institution"
      },
      "outputs": [],
      "source": [
        "sigma_min = 0.3\n",
        "\n",
        "# This setup previously gave an error\n",
        "initial_weights = np.array([0.995, 0.005])\n",
        "initial_mus = np.array([[0, 0], [-4, 1]]) # The second component sits on top of the outlying pooint\n",
        "initial_sigmas = np.array([[4, 4], [1, 1]]) # The second component is quite narrow\n",
        "\n",
        "initial_theta = initial_weights, initial_mus, initial_sigmas\n",
        "\n",
        "trained_theta, training_curve = train_gmm(X_train, *initial_theta, n_iter=n_iter, sigma_min=sigma_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "angry-regard",
      "metadata": {
        "id": "angry-regard"
      },
      "outputs": [],
      "source": [
        "n_components = 10 # This previously gave an error\n",
        "n_iter = 30\n",
        "sigma_min = 0.3\n",
        "\n",
        "initial_theta = init_gmm(X_train, n_components)\n",
        "\n",
        "trained_theta, training_curve = train_gmm(X_train, *initial_theta, n_iter, sigma_min=sigma_min)\n",
        "\n",
        "trained_weights, trained_mus, trained_sigmas = trained_theta\n",
        "\n",
        "print(\"Trained weights: \" + str(trained_weights))\n",
        "print(\"Trained mus: \" + str(trained_mus))\n",
        "print(\"Trained sigmas: \" + str(trained_sigmas))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "instrumental-cause",
      "metadata": {
        "id": "instrumental-cause"
      },
      "source": [
        "Hooray! Problem solved!\n",
        "\n",
        "But it should be noted that variance flooring isn't the only way to fix this problem. If you recall how we were able to use use prior distributions to constrain the parameter estimates on the 1D problem, the same approach can also be used here. There are, in fact, elegant Bayesian versions of the EM-algorithm for, e.g., GMMs, that you might see in future courses, but we will not go into those here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "concerned-advertising",
      "metadata": {
        "id": "concerned-advertising"
      },
      "source": [
        "### Take-home message\n",
        "\n",
        "Variance flooring is a common fix/regulariser in many classic probabilistic models. It illustrates some failure modes that are particularly common in probabilistic modelling, namely loss/lack of numerical precision and degenerate optima.\n",
        "\n",
        "That's not to say deep learning can't face these issues too. As an example, the combination of cross-entropy loss and softmax layers (used in the vast majority of neural-network-based classifiers) is likely to lead to loss of numerical precision, which is why we have a special command `torch.nn.BCEWithLogitsLoss` in PyTorch that combines the two in a numerically safe way. And one reason that deep neural network training is less likely to lead to degeneracies might be that neural nets are _harder_ to optimise well, which makes it more unlikely that the optimiser will find those points where the objective function attains extreme values due to degenerate optima!\n",
        "\n",
        "However, theres also a wider story. The cycle of problem and solution that you saw here is a good example of a common pattern for how many different machine-learning methods behave in practice:\n",
        "1. There's a beautiful theoretical formulation of how machine learning is supposed to work.\n",
        "2. Once we apply the theory, it turns out that it doesn't always work reliably in applications, at least not straight off the bat.\n",
        "3. To make methods work consistently (and work well) in practice, people invent a few more-or-less hacky “tricks” to fix the issues.\n",
        "4. While many of these tricks come start out as quick-and-dirty engineering fixes, made to prevent implementational issues arising deep within the code, some of them can be given a theoretical interpretation if we think about them a bit more.\n",
        "5. Sometimes a machine-learning researcher will look at these supposed hacks and their theoretical interpretation, and use these as a stepping stone to develop a better theory of machine learning and/or the problem at hand.\n",
        "\n",
        "This feedback loop between theory and practice is an important factor in driving the field of machine learning forwards."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "advanced-syria",
        "concerned-advertising"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}