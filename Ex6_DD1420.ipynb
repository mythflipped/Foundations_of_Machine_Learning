{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDGJPL5HC0ch"
      },
      "source": [
        "# General Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bacf29"
      },
      "source": [
        "Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the [Code of Conduct](https://kth.instructure.com/courses/32018/pages/code-of-conduct). In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions.\n",
        "\n",
        "If you need assistance with the exercise, you are encouraged to post a question to the appropriate [Discussion Topic](https://kth.instructure.com/courses/32018/discussion_topics) or sign up for a help session.\n",
        "\n",
        "<br>\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and delete the `raise NotImplementedError()` once you have implemented the solution\n",
        "\n",
        "<br>\n",
        "\n",
        "You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all` then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows\n",
        "\n",
        "<br>\n",
        "\n",
        "`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`\n",
        "\n",
        "<br>\n",
        "\n",
        "where you replace `??` with the correct exercise number. If you are working alone you do not need to include a partner name. Correctly naming the file and including your name (and your partner's) below is worth **1 point** - if you fail to correctly name the file or include your partner's name, *you will lose this point*.\n",
        "\n",
        "<br>\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g-7KY-KlWMR"
      },
      "source": [
        "# Name (1 pts)\n",
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly): <font color=\"red\"> (1 Point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PS4iHP5lY4q"
      },
      "source": [
        "\n",
        "### Student: LAST, FIRST\n",
        "\n",
        "### Partner: LAST, FIRST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYzQ4f3klWij"
      },
      "source": [
        "# Exercise 6: Machine learning and kernel methods (25 Points Total)\n",
        "\n",
        "In this exercise, we will work practically with the Kernelization of Linear Regression that we discussed in the lecture notes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5leDQRLlbSd"
      },
      "outputs": [],
      "source": [
        "# General imports for the exercise\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Fixing the random seed for reproducibility\n",
        "RANDOM_SEED=4321\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOsEvZ-TsPy2"
      },
      "source": [
        "# 9. Kernel Linear Regression (6P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNGWe4c6_38t"
      },
      "source": [
        "**9.1** Use the definition of a kernel to explain why kernels are symmetric: $k(x,x') = k(x',x)$. What underlying property is the reason? 1 sentence answer is enough. $\\color{red}{\\text{(2 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-b2CrBn_38u"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhMFkIni_38v"
      },
      "source": [
        "**9.2** One way to check if a given function is a kernel is to show that it can be decomposed into the inner product in some feature space, i.e. $\\langle \\phi(x), \\phi(x') \\rangle$. Given a two-dimensional real-valued input space $x=(x_1, x_2)$, show that $k(x,x')= (x^T x')^2$ is a valid kernel by identifying the corresponding feature map $\\phi(x).$  $\\color{red}{\\text{(4 points)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xviiKGKN_38v"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxUqDXLVlntN"
      },
      "source": [
        "Consider the dataset that is plotted when you run the next cell. Our goal will be to implement ourselves both primal and dual versions of kernelized regression in order to find a good fit to this data and test our understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nZTxpUR9l4rt",
        "outputId": "50b41e05-c7b1-48dc-e4a8-e22ecb110334"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate noisy observations\n",
        "gen_data = lambda t, c : np.sum([a * (t[:,0]**i) for i, a in enumerate(c)], axis=0)\n",
        "\n",
        "n = 50\n",
        "coeff = [-2, 3, 4, -2,-5]\n",
        "X = np.random.uniform(-1.0, 1.0, n).reshape(-1,1)\n",
        "y = gen_data(X, coeff) + np.random.normal(0.0, 0.2, n)\n",
        "\n",
        "# Plot data\n",
        "plt.scatter(X, y, label=\"data\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT-3UchZ2QD9"
      },
      "source": [
        "**9.3**  Take a look at the documentation of `sklearn.kernel_ridge.KernelRidge` online. This module implements the dual kernelized linear regression we discussed in the lectures.\n",
        "\n",
        "a) Complete the function `fit_ridge` below to returns the trained model following the documentation of sklearn.kernel_ridge.KernelRidge. Use the next cell after to visualize what this function does. Use a polynomial kernel. $\\color{red}{\\text{(2 points)}}$\n",
        "\n",
        "b) What parameters for degree d and l yield a good fit? $\\color{red}{\\text{(2 points)}}$\n",
        "\n",
        "c) What happens if you now increase or decrease the value of l?  $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "d) What polynomial p(x) do we converge to as the regularization parameter l tends to +infinity? $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OGo6xvH_38y"
      },
      "source": [
        "YOUR ANSWER TO b), c) , d) HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zk1rwux8Tsx"
      },
      "outputs": [],
      "source": [
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "def fit_ridge(X, y, l, d):\n",
        "    \"\"\"\n",
        "    X: np.ndarray of shape [N, 1]\n",
        "    y: np.ndarray of shape [N]\n",
        "    l: regularization parameter: real value\n",
        "    d: degree of polynomial\n",
        "    Returns:\n",
        "        model: object of type KernelRidge\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE for a) HERE (hint: you can do it in just 2 lines)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvpkkOpZ8G1_"
      },
      "outputs": [],
      "source": [
        "# Predict for interval (-1.0, 1.0)\n",
        "X_pred = np.linspace(-1.0, 1.0, 100).reshape(-1,1)\n",
        "# Set your regularization parameter here:\n",
        "l = 0.1\n",
        "degree= 3\n",
        "# Fit kernel regression model\n",
        "y_pred_kernel = fit_ridge(X, y, l, degree).predict(X_pred)\n",
        "\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, y, label=\"data\")\n",
        "plt.plot(X_pred, y_pred_kernel, c='orange', label=\"kernel LR\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qio9LFPo_380"
      },
      "source": [
        "**9.4** Let us implement our own version of fit_ridge from scratch using the formulas for the optimal parameters $\\theta$ that we derived in the notes. Hint: use np.eye(n) to create an identity matrix of size n by n.\n",
        "The function Phi below already implements the matrix Phi for the polynomials of up to degree d feature maps from the lecture notes.\n",
        "\n",
        "a) Implement theta_primal which returns the optimal theta vector from the lecture using the primal solution for linear regression with a feature map. ${\\text{(2 points)}}$\n",
        "\n",
        "b) Implement theta_dual which implements the optimal theta vector from the lecture using the dual solution for the linear regression with a feature map. This should give the same result as theta_primal, of course. ${\\text{(2 points)}}$\n",
        "\n",
        "c) Use the prepared code in the next two cells to compute y_pred_primal: the predicted y values from the theta_primal and y_pred_dual: the predicted y values from theta_dual. You should obtain two identical plots which are very similar to the plot above (at least for small regularization). ${\\text{(2 points)}}$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koLbT7E8_381"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def Phi(X, d):\n",
        "    \"\"\"\n",
        "    X: np.ndarray of shape [N, 1]\n",
        "    Returns:\n",
        "      Phi matrix from lecture corresponding to polynomial feature map,\n",
        "      np.ndarray of shape [N, d+1]\n",
        "    \"\"\"\n",
        "    poly = PolynomialFeatures(d)\n",
        "    return poly.fit_transform(X)\n",
        "\n",
        "\n",
        "def theta_primal(Phi, Ys, l):\n",
        "    \"\"\"\n",
        "    Phi: our matrix Phi as in the lecture notes containing rows of phi(x_i) shape [N, k]\n",
        "    Ys: predicted Y values of shape [N]\n",
        "    l: real valued regularization parameter\n",
        "    Returns: Optimal primal solution to theta, vector in dimension k\n",
        "    \"\"\"\n",
        "    # YOUR_CODE_HERE (can be done in 1-2 lines, but you can expand also)\n",
        "\n",
        "def theta_dual(Phi, Ys, l):\n",
        "    \"\"\"\n",
        "    Phi: our matrix capital Phi as in the lecture notes containing rows of phi(x_i) shape [N, k]\n",
        "    Ys: predicted Y values of shape [N]\n",
        "    l: real valued regularization parameter\n",
        "    Returns: Optimal primal solution to theta, vector in dimension k\n",
        "    \"\"\"\n",
        "    # YOUR_CODE_HERE (can be done in 1-2 lines, but you can expand also)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn0Q4ZN2_381"
      },
      "outputs": [],
      "source": [
        "# Predict for interval (-1, 1)\n",
        "X_pred = np.linspace(-1.0, 1.0, 100).reshape(-1,1)\n",
        "# Set your regularization parameter here:\n",
        "l = 0.1\n",
        "degree= 3\n",
        "# implement the estimated function value computation at X_pred: y_pred_primal = ... (should involve theta_primal)\n",
        "y_pred_primal = YOUR_CODE_HERE\n",
        "# Plot results\n",
        "plt.scatter(X, y, label=\"data\")\n",
        "plt.plot(X_pred, y_pred_primal, c='orange', label=\"kernel LR\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9nWFYsf_382"
      },
      "outputs": [],
      "source": [
        "# Predict for interval (-1, 1)\n",
        "X_pred = np.linspace(-1.0, 1.0, 100).reshape(-1,1)\n",
        "# Set your regularization parameter here:\n",
        "l = 0.1\n",
        "degree = 3\n",
        "# implement the estimated function value computation at X_pred: y_pred_ = ... (should involve theta_primal)\n",
        "y_pred_dual = YOUR_CODE_HERE\n",
        "# Plot results\n",
        "plt.scatter(X, y, label=\"data\")\n",
        "plt.plot(X_pred, y_pred_dual, c='orange', label=\"kernel LR\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cZaFXck_382"
      },
      "source": [
        "**9.5** In the last exercise we have seen practically that both primal and dual solutions to the optimal theta parameter from the lecture notes indeed yield the same result. Now we focus on the dual formulation and rewrite it using the kernel trick so that we can work with kernel functions directly.\n",
        "\n",
        "Have a look at the skeleton code below. It implements the solution of kernel regression using the kernel trick.\n",
        "\n",
        "a) Implement the function Gram that computes a Gram matrix for an input kernel function depending on a parameter as described. ${\\text{(2 points)}}$\n",
        "\n",
        "b) Implement the function kernel_poly. The resulting plot for identical degree and regularization parameters as before should look as the result in the code that used kernel ridge regression from the python ligrary but this time we computed it ourselves using only kernel evaluations. ${\\text{(2 points)}}$\n",
        "\n",
        "c) Implement the function kernel_gauss. Experiment with different sigma values - can you find suitable values and what is the result of changing these?  ${\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWQUhXjj_382"
      },
      "outputs": [],
      "source": [
        "def kernel_poly(x, y, degree):\n",
        "    \"\"\"\n",
        "    x,y: np.ndarray of shape [N, 1] (vectors of dim n)\n",
        "    degree: integer, degree kernel parameter of polynomial kernel\n",
        "    Returns:\n",
        "        real valued Kernel value of polynomial kernel with degree d evaluated at x and y\n",
        "    \"\"\"\n",
        "    # YOUR_CODE_HERE (can be done in 1 line)\n",
        "\n",
        "\n",
        "def kernel_gauss(x, y, sigma):\n",
        "    \"\"\"\n",
        "    x,y: np.ndarray of shape [N, 1] (vectors of dim n)\n",
        "    sigma: real value, sigma kernel parameter of Gaussian kernel\n",
        "\n",
        "    Returns:\n",
        "        real valued kernel value of Gaussian kernel with parameter sigma evaluated at x and y\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE (can be one in 1 line)\n",
        "\n",
        "\n",
        "def Gram(X, kernel, kernel_param):\n",
        "    \"\"\"\n",
        "    X: np.ndarray of shape [N, 1]\n",
        "    kernel: a kernel *function* with the same input/output signature as kernel_poly and kernel_gauss above\n",
        "    kernel_param: real parameter for the kernel function, e.g. degree for kernel_poly and sigma for kernel_gauss\n",
        "    Returns:\n",
        "      The Gram matrix for the kernel with parameter set to kernel_param: np.ndarray of shape [N, N]\n",
        "    \"\"\"\n",
        "    # YOUR IMPLEMENTATION HERE (can be done in 5 lines)\n",
        "\n",
        "\n",
        "# Recall k(x) from the lecture? This evaluates this function on all X_pred[j] and returns a matrix of results\n",
        "def k_vectors(X, X_pred, kernel, kernel_param):\n",
        "    result = np.zeros([X.shape[0], X_pred.shape[0]])\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X_pred.shape[0]):\n",
        "            result[i, j] = kernel(X[i], X_pred[j], kernel_param)\n",
        "    return result\n",
        "\n",
        "# Determines the solution y_pred prediction value given input data and target X_pred values where we need to\n",
        "# evaluate the solution.\n",
        "def kernel_solution(X, y, X_pred, kernel, kernel_param, l):\n",
        "    inverse = y.dot(np.linalg.inv(Gram(X, kernel, kernel_param) + l * np.eye(X.shape[0])))\n",
        "    return inverse.dot(k_vectors(X, X_pred, kernel, kernel_param))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7BZvShw_383"
      },
      "outputs": [],
      "source": [
        "# Predict for interval (-1, 1)\n",
        "X_pred = np.linspace(-1.0, 1.0, 100).reshape(-1,1)\n",
        "\n",
        "l = 0.1\n",
        "degree= 3\n",
        "sigma = 0.03\n",
        "y_pred_poly = kernel_solution(X, y, X_pred, kernel_poly, degree, l)\n",
        "y_pred_gauss = kernel_solution(X, y, X_pred, kernel_gauss, sigma, l)\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, y, label=\"data\")\n",
        "plt.plot(X_pred, y_pred_poly, c='orange', label=\"kernel LR\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(X, y, label=\"data\")\n",
        "plt.plot(X_pred, y_pred_gauss, c='orange', label=\"kernel LR\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw6STVjn_384"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3PS4iHP5lY4q"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}