{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee85c789",
      "metadata": {
        "id": "ee85c789"
      },
      "source": [
        "# General Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f20e29",
      "metadata": {
        "id": "b9f20e29"
      },
      "source": [
        "Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the [Code of Conduct](https://kth.instructure.com/courses/32018/pages/code-of-conduct). In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions.\n",
        "\n",
        "If you need assistance with the exercise, you are encouraged to post a question to the appropriate [Discussion Topic](https://kth.instructure.com/courses/32018/discussion_topics) or sign up for a help session.\n",
        "\n",
        "<br>\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and delete the `raise NotImplementedError()` once you have implemented the solution\n",
        "\n",
        "<br>\n",
        "\n",
        "You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all` then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows\n",
        "\n",
        "<br>\n",
        "\n",
        "`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`\n",
        "\n",
        "<br>\n",
        "\n",
        "where you replace `??` with the correct exercise number. If you are working alone you do not need to include a partner name. Correctly naming the file and including your name (and your partner's) below is worth **1 point** - if you fail to correctly name the file or include your partner's name, *you will lose this point*.\n",
        "\n",
        "<br>\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b15e14",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4e10dd9ff9316230e586bc2ddf7c902a",
          "grade": false,
          "grade_id": "cell-d2aae5414ee22e91",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "61b15e14"
      },
      "source": [
        "# Name (1 pts)\n",
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly): <font color=\"red\"> (1 Point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3df5672",
      "metadata": {
        "id": "d3df5672"
      },
      "source": [
        "\n",
        "### Student: LAST, FIRST\n",
        "\n",
        "### Partner: LAST, FIRST"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875b3d26",
      "metadata": {
        "id": "875b3d26"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9690bed0",
      "metadata": {
        "id": "9690bed0"
      },
      "source": [
        "# Exercise 9: Machine learning and information theory (16 points)\n",
        "\n",
        "In this exercise, you will implement and study the behaviour of a number of concepts from module 9 in the course. The exercise contains two parts:\n",
        "1. Implementing basic concepts from information theory\n",
        "2. Implementing, training, and tuning decision trees\n",
        "\n",
        "_Important:_<br/>\n",
        "Make sure you have [graphviz](https://graphviz.org/download/) installed. It will be used to visualise decision trees. It is already available in the Google Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c215309",
      "metadata": {
        "id": "8c215309"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install numpy matplotlib seaborn scikit-learn scipy pydot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4696e3ba",
      "metadata": {
        "id": "4696e3ba"
      },
      "outputs": [],
      "source": [
        "# Useful imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import pydot\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "from uuid import uuid4\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Initial plotting function\n",
        "def plot_function(function, inputs, title=None, ylabel=None):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(inputs, function(inputs))\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    ax.set_xticks(np.linspace(0, 1, 11))\n",
        "    ax.set_xlabel(\"Probability of $x$\")\n",
        "    if ylabel:\n",
        "        ax.set_ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12948d59",
      "metadata": {
        "id": "12948d59"
      },
      "source": [
        "## 9.1 General information theory (6.25 Points)\n",
        "\n",
        "In this first part of the exercise, you will implement and play with a few information-theoretic concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60312d13",
      "metadata": {
        "id": "60312d13"
      },
      "source": [
        "### Information content\n",
        "\n",
        "The _Shannon information_ or _information content_ is a measure of how surprising an event is. It is given by the following formula\n",
        "$$ I(x) = \\log_2 \\frac{1}{P(\\boldsymbol x=x)}\\ = - \\log_2 P(\\boldsymbol x=x) $$\n",
        "Since we are using the 2-logarithm, this gives the entropy in bits.\n",
        "\n",
        "Mathematically we see that something that is completely certain ($P(\\boldsymbol x=x)=1$) has no new information in it at all ($I(x)=0$). According to the definition, the more unlikely an event is, the more information it contains. The Shannon information is therefore also known as the _surprisal_.\n",
        "\n",
        "**9.1.1 Task:**<br/>\n",
        "Implement a function, `information_content`, to compute the information content in bits according to the above formula, given the probability `p_x` of an event. <font color=\"red\"> (0.25 point) </font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "The 2-logarithm can be accessed through `np.log2`. This function also works on arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e78eef",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1ab12179743c4f6b60f5ed5b584a7ffa",
          "grade": false,
          "grade_id": "cell-aeef7809f7bbbd79",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "02e78eef"
      },
      "outputs": [],
      "source": [
        "def information_content(p_x):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57753c7b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cfbc43c7a79df3994318f0f48275d651",
          "grade": true,
          "grade_id": "cell-9a906fe7e2fd8ca1",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "57753c7b"
      },
      "outputs": [],
      "source": [
        "assert information_content(0.5) == 1\n",
        "assert round(information_content(0.01), 2) == 6.64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a3e24c",
      "metadata": {
        "id": "22a3e24c"
      },
      "source": [
        "The code below plots a graph of the surprisal as a function the event probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6137e8c2",
      "metadata": {
        "id": "6137e8c2"
      },
      "outputs": [],
      "source": [
        "p_x = np.linspace(1, 0, 1001, endpoint=False)[1:] # Probabilities on (0, 1)\n",
        "plot_function(information_content, p_x, title=\"Probability vs. information\", ylabel=\"Information $I(x)$ (bits)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ff939f4",
      "metadata": {
        "id": "4ff939f4"
      },
      "source": [
        "### Entropy\n",
        "\n",
        "The _information entropy_ or _Shannon entropy_ (often just “entropy”, for short) is perhaps the most widely used concept in information theory. It is defined as the expected information content of a random variable $\\boldsymbol x$. The entropy in bits can be computed as follows\n",
        "$$ H(\\boldsymbol x) = \\mathbb{E}_{\\boldsymbol x} \\left[ - \\log_2 P(\\boldsymbol x) \\right] = - \\sum_{x\\in \\Omega} P(\\boldsymbol x=x) \\log_2 P(\\boldsymbol x=x)$$\n",
        "\n",
        "**9.1.2 Task:**<br/>\n",
        "Implement the function `entropy_bernoulli` that computes the information entropy of a Bernoulli random variable $\\mathbf x$, with the probability mass function\n",
        "$$ P(\\mathbf x=\\mathrm x) = \\begin{cases} \\theta &\\text{if $\\mathrm x=1$}\\\\\n",
        "1 - \\theta &\\text{if $\\mathrm x=0$,}\n",
        "\\end{cases} $$\n",
        "as a function of the success probability $\\theta=$ `p_x`. `p_x` can be an array, in which case the each element is an different value of $\\theta$ for which to compute the Bernoulli entropy. <font color=\"red\"> (0.5 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e17a2e1",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c7009b91132a4d7df62b63e7811c96fd",
          "grade": false,
          "grade_id": "cell-748f53995a634331",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2e17a2e1"
      },
      "outputs": [],
      "source": [
        "def entropy_bernoulli(p_x):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33667c49",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "24f62fe3f7ef50c0d3e738a5b4ceca8d",
          "grade": true,
          "grade_id": "cell-f081b2453f1ce8d1",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "33667c49"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "assert entropy_bernoulli(0.5) == 1\n",
        "assert np.round(entropy_bernoulli(0.001), 2) == 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd112eb",
      "metadata": {
        "id": "0fd112eb"
      },
      "source": [
        "What happens if $\\theta=0$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc9b7ad",
      "metadata": {
        "id": "5bc9b7ad"
      },
      "outputs": [],
      "source": [
        "entropy_bernoulli(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f57050",
      "metadata": {
        "id": "31f57050"
      },
      "source": [
        "It is likely that your function throws an error or returns a value like `nan` when you run the above code. This is because $-\\log_2 P(\\mathbf x=\\mathrm x)\\to\\infty$ as $P(\\mathbf x=\\mathrm x)\\to 0$. The entropy is thus a product of something that's zero and something that is infinite.\n",
        "\n",
        "**9.1.3 Task:**<br/>\n",
        "Let's call $P(\\mathbf x=\\mathrm x)=p$. Evaluate the limit $I(0)=\\lim_{p\\to 0} -p \\ln p$ as $p$ approaches zero from above (i.e., $p>0$). Show your work in the below cell. <font color=\"red\"> (1 point) </font>\n",
        "\n",
        "_Note:_<br/>\n",
        "It is not sufficient to just state the answer. You have to do a brief derivation based on what you learned in calculus class to show your work.\n",
        "\n",
        "_Hint:_<br/>\n",
        "There are many ways to do this. You can, for instance, use L'Hôpital's rule."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d0f0111",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e3cf197c4889d3e823aa4e6bf1341dd6",
          "grade": true,
          "grade_id": "cell-dff1e94d0eba4bf2",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "2d0f0111"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca33a4d0",
      "metadata": {
        "id": "ca33a4d0"
      },
      "source": [
        "Now you both know why $\\theta=0$ gave an error, and what the value should be in that case.\n",
        "\n",
        "**9.1.4 Task:**<br/>\n",
        "Implement a modified version of `entropy_bernoulli` that also handles the edge cases $\\theta=0$ and $\\theta=1$ correctly.\n",
        "\n",
        "_Hint:_<br/>\n",
        "You can use the command `np.where` to give a special treatment to elements of `p_x` that satisfy a boolean condition of your choice. Even if your solution is correct, you may still see a runtime warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0536996",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d556785b284fc1fcf21eae555b7702a1",
          "grade": false,
          "grade_id": "cell-7ab97a1ae4a306f9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "e0536996"
      },
      "outputs": [],
      "source": [
        "def entropy_bernoulli(p_x):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf49395",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "566f267bcb84b8d723ffce3e47b75d00",
          "grade": true,
          "grade_id": "cell-a3ba613a80aee11d",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3cf49395"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "assert entropy_bernoulli(0) == 0\n",
        "assert entropy_bernoulli(1) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06df9cd4",
      "metadata": {
        "id": "06df9cd4"
      },
      "source": [
        "Let's plot the information entropy as a function of $\\theta\\in[0,1]$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04790896",
      "metadata": {
        "id": "04790896"
      },
      "outputs": [],
      "source": [
        "p_x = np.linspace(0, 1, 1001)\n",
        "plot_function(entropy_bernoulli, p_x,\n",
        "              \"Entropy of a Bernoulli distributed random variable\", \"Information entropy $H(x)$ (bits)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f032bca8",
      "metadata": {
        "id": "f032bca8"
      },
      "source": [
        "If you have done things right, you should see a roughly parabolic, upside down curve. But is it an actual parabola? Let's find out!\n",
        "\n",
        "**9.1.5 Task:**<br/>\n",
        "Derive an equation for the parabola $f(\\mathrm x)$ that passes through the three points $(0,0)$, $(\\frac{1}{2},1)$, and $(1,0)$. Show your solution and your work in the cell below. <font color=\"red\"> (1 point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b6991ef",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "112b5b1588d52531bec18b4482ca2ca8",
          "grade": true,
          "grade_id": "cell-a0a29641f697dc39",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7b6991ef"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8706bfd2",
      "metadata": {
        "id": "8706bfd2"
      },
      "source": [
        "**7.1.6 Task:**<br/>\n",
        "Plot the parabola you just derived on the interval $[0, 1]$, overlaid on the previous function `entropy_bernoulli` plotted on the same interval, so that you can compare the two curves. <font color=\"red\"> (0.5 point) </font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "You can borrow some plotting code from `plot_function` defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb55808",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cece99de70ccdfacd6d93a995fdfef61",
          "grade": true,
          "grade_id": "cell-3267b0fd09458013",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "4eb55808"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3090b56",
      "metadata": {
        "id": "c3090b56"
      },
      "source": [
        "It looks like the parabola always is below the information entropy $H$, although the difference is moderate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f168be2c",
      "metadata": {
        "id": "f168be2c"
      },
      "source": [
        "Did you notice that the entropy in the plot never seems to go below zero? That is no coincidence.\n",
        "\n",
        "**9.1.7 Task:**<br/>\n",
        "Prove that $H(\\boldsymbol x)\\geq 0$ (entropy is never negative) for _any_ discrete random variable $\\boldsymbol x$. <font color=\"red\"> (1 point) </font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "Use the fact that $0\\leq P(\\boldsymbol x)\\leq 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779806cc",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0bd5efc83085c7909e796584f056ed2d",
          "grade": true,
          "grade_id": "cell-f581e23aed85ed72",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "779806cc"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23fbe0f7",
      "metadata": {
        "id": "23fbe0f7"
      },
      "source": [
        "### The Kullback-Leibler divergence\n",
        "\n",
        "Thus far we have only considered functions of a single distribution. In contrast, the _Kullback-Liebler divergence_ or _relative entropy_ (often abbeviated KLD) is a function of two distributions $P$ and $Q$, that quantifies how much one differs from the other. The KLD in bits is given by\n",
        "$$ D_{\\mathrm{KL}}(P||Q)\n",
        "= \\sum_{\\mathrm x\\in \\Omega} \\left(P(\\mathbf x=\\mathrm x) \\log_2 \\frac{P(\\mathbf x=\\mathrm x)}{Q(\\mathbf x=\\mathrm x)} \\right)\n",
        "= \\mathbb{E}_{P_{\\mathbf x}}\\left[\\log_2 \\frac{P(\\mathbf x)}{Q(\\mathbf x)}\\right]$$\n",
        "\n",
        "A few properties of the KLD are:\n",
        "* It is zero if $P$ and $Q$ are the same distribution\n",
        "* It is greater than zero if $P$ and $Q$ are not same distribution\n",
        "* It is asymmetric: the _forward_ KLD $D_{\\mathrm{KL}}(P||Q)$ does not equal the _reverse_ KLD $D_{\\mathrm{KL}}(Q||P)$ in general"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c61887",
      "metadata": {
        "id": "55c61887"
      },
      "source": [
        "**9.1.8 Task:**<br/>\n",
        "Implement the function `kl_divergence_discrete` that computes $D_{\\mathrm{KL}}(P||Q)$ in bits between `p` and `q`, where `p` and `q` are `np.array`s of the same length that describe two distributions (i.e., they list the probabilities of each outcome, in order). <font color=\"red\"> (1 point) </font>\n",
        "\n",
        "_Important:_<br/>\n",
        "Use `np.where`, like you did above, to avoid issues if $P(\\mathbf x=\\mathrm x)=0$ for any $\\mathrm x$.\n",
        "\n",
        "_Hint:_<br/>\n",
        "You can assume that `p` and `q` both are nonnegative and sum to one; no error checking needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e834588",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8fc2dc34844f9da186b681deb35061af",
          "grade": false,
          "grade_id": "cell-1dd595cc4b417650",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0e834588"
      },
      "outputs": [],
      "source": [
        "def kl_divergence_discrete(p, q):\n",
        "    with np.errstate(all='ignore'):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b5f1d8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a94f7ba150871d386e042351bbc3803d",
          "grade": true,
          "grade_id": "cell-ae4b14b8eef5a571",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "e0b5f1d8"
      },
      "outputs": [],
      "source": [
        "assert not np.isnan(kl_divergence_discrete(np.array([0, 0, 0.7, 0.3]), np.array([0.1, 0.2, 0.4, 0.3]))), \"Handle the case when any item of p is 0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701f1753",
      "metadata": {
        "id": "701f1753"
      },
      "source": [
        "What happens if $P(\\mathbf x=\\mathrm x)>0$ but $Q(\\mathbf x=\\mathrm x)=0$ for some $\\mathrm x$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba605ede",
      "metadata": {
        "id": "ba605ede"
      },
      "outputs": [],
      "source": [
        "kl_divergence_discrete(np.array([0.7, 0.3]), np.array([1, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cb7fa74",
      "metadata": {
        "id": "6cb7fa74"
      },
      "source": [
        "The KLD can also be generalised to continuous distributions, in such a way that the previous properties still hold. The associated formulas (in bits) are\n",
        "$$ D_{\\mathrm{KL}}(P||Q)\n",
        "= \\int_{\\Omega} \\left(p_{\\boldsymbol x}(x) \\log_2 \\frac{p_{\\boldsymbol x}(x)}{q_{\\boldsymbol x}(x)} \\right) \\mathrm dx\n",
        "= \\mathbb{E}_{p_{\\boldsymbol x}}\\left[\\log_2 \\frac{p_{\\boldsymbol x}(\\boldsymbol x)}{q_{\\boldsymbol x}(\\boldsymbol x)}\\right]$$\n",
        "\n",
        "Below is a pre-implemented function `kl_divergence_continous` that uses Riemann integration (with step size `bin_width`) to compute an approximate KLD between two densities evaluated at the same, equidistant points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc3924d",
      "metadata": {
        "id": "4cc3924d"
      },
      "outputs": [],
      "source": [
        "def kl_divergence_continous(dist1, dist2, bin_width):\n",
        "    # We make sure that the dist1 and dist2 are probability distributions so they should integrate to 1\n",
        "    # Think how Riemann integral works? We approximate values with rectangles whose width tends to 0\n",
        "    # In our case we have bin_width given as parameter and the height of the bins are given by dist1 and dist2\n",
        "    if not all([np.isclose(dist2.sum() * bin_width, 1, atol=1e-2), np.isclose(dist1.sum() * bin_width, 1, atol=1e-2)]):\n",
        "        warnings.warn(\"One of the input distributions has significant probability mass outside the area specified in the problem thus the KL divergence might be incorrect.\")\n",
        "\n",
        "    # Normalize the distribution to sum to 1 and name them p and q\n",
        "    p = dist1 * bin_width\n",
        "    q = dist2 * bin_width\n",
        "\n",
        "    assert all([np.isclose(p.sum(), 1, atol=1e-1), np.isclose(q.sum(), 1, atol=1e-1)]), \"One of the distribution does not sums to 1. or close to 1 upto one precision\"\n",
        "\n",
        "    return kl_divergence_discrete(p, q)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb4eae1",
      "metadata": {
        "id": "3eb4eae1"
      },
      "source": [
        "We now define a test problem where $p_{\\boldsymbol x}(x)$ is a GMM, which we are trying to approximate using a Gaussian density $q_{\\boldsymbol x}(x)$ with adjustable mean $\\mu$ and standard deviation $\\sigma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e25f525",
      "metadata": {
        "id": "4e25f525"
      },
      "outputs": [],
      "source": [
        "def gmm_vs_gauss_kld(mu=None, sigma=None):\n",
        "\n",
        "    # Data distribution P(z)\n",
        "    begin, end= -15, 15\n",
        "    x = np.linspace(begin, end, 1000)\n",
        "    bin_width = x[1] - x[0]\n",
        "    norm1 = stats.norm.pdf(x, loc=3, scale=1)\n",
        "    norm2 = stats.norm.pdf(x, loc=-3, scale=1)\n",
        "    p_z = (norm1 + norm2) / 2\n",
        "\n",
        "    fig, axis = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
        "    title_all = [\"Forward KL Divergence\", \"Reverse KL Divergence\"]\n",
        "    reverse_KL = [False, True]\n",
        "\n",
        "    for i, ax in enumerate(axis):\n",
        "\n",
        "        ax.plot(x, p_z, label=\"$P(z)$\")\n",
        "        ax.set_xticks(np.arange(begin, end))\n",
        "        ax.fill_between(x, p_z, alpha=0.3)\n",
        "        title = title_all[i]\n",
        "\n",
        "        if mu is not None and sigma is not None:\n",
        "            assert isinstance(mu, (int, float)) and isinstance(sigma, (int, float)), \"Inputs should be numbers.\"\n",
        "            assert begin <= mu <= end, f\"Mean should be between {begin} and {end}. as we are integrating over this area if the probability mass will go beyond these points, the integral of probability will not sum to 1.\"\n",
        "\n",
        "            q_z = stats.norm.pdf(x, loc=mu, scale=sigma)\n",
        "            ax.plot(x, q_z, label=\"$Q(z)$\")\n",
        "            ax.fill_between(x, q_z, alpha=0.3)\n",
        "            kl_div = kl_divergence_continous(p_z, q_z, x[1] - x[0]) if not reverse_KL[i] else kl_divergence_continous(q_z, p_z, x[1] - x[0])\n",
        "            title += f\" $KL(P||Q)$ = {kl_div: .2f}\" if not reverse_KL[i] else f\" $KL(Q||P)$ = {kl_div: .2f}\"\n",
        "\n",
        "        ax.set_title(title)\n",
        "        ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d38e0f09",
      "metadata": {
        "id": "d38e0f09"
      },
      "source": [
        "Since the GMM is non-Gaussian, the approximation will not be perfect, no matter what parameter values we choose for $q_{\\boldsymbol x}(x)$. The KLD will never be zero. However, it is quite instructive to see what the best approximation looks like. What happens when we cannot be perfect?\n",
        "\n",
        "**9.1.9 Task:**<br/>\n",
        "The code below plots $p_{\\boldsymbol x}(x)$ and $q_{\\boldsymbol x}(x)$, and computes two different Kullback-Leibler divergences between them. Experiment with the values of `mu` and `sigma` for $q_{\\boldsymbol x}$ until you identify values that make the _reverse KLD_ (bottom plot) as small as possible. <font color=\"red\"> (0.25 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80315566",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d0efcb4c6e9c5140eab593550f8f2058",
          "grade": true,
          "grade_id": "cell-375a308c8008451a",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "80315566"
      },
      "outputs": [],
      "source": [
        "# Change the values of the variables mu and sigma here until you have a good solution\n",
        "mu = 8\n",
        "sigma = 1\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "gmm_vs_gauss_kld(mu, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ad71a7",
      "metadata": {
        "id": "28ad71a7"
      },
      "source": [
        "This is known as _mode-seeking behaviour._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957f5ac9",
      "metadata": {
        "id": "957f5ac9"
      },
      "source": [
        "**9.1.10 Task:**<br/>\n",
        "Experiment with the values of `mu` and `sigma` for $q_{\\boldsymbol x}$ until you identify values that make the _forward KLD_ (top plot) as small as possible. <font color=\"red\"> (0.25 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacbed17",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6397284fbf1414698ea8918ea14a2c98",
          "grade": true,
          "grade_id": "cell-025e0ff8c95c437f",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bacbed17"
      },
      "outputs": [],
      "source": [
        "# Change the values of the variables mu and sigma here until you have a good solution\n",
        "mu = 8\n",
        "sigma = 1\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "gmm_vs_gauss_kld(mu, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7117c40",
      "metadata": {
        "id": "f7117c40"
      },
      "source": [
        "This is called _mass-covering behaviour_.\n",
        "\n",
        "When a parametric model is misspecified (as seen here) and cannot fit the true data, our objective function will decide how it behaves, i.e., in what way it “perfers to fail”. The two behviours above are especially common failure modes of probabilistic models. The case of forward-KLD minimisation, in particular, is a toy-problem analogue for what happens when we use maximum likelihood to (as is typical) fit a too simple model to a complicated real-world dataset: the model will become too broad and smeared out, compared to the true, underlying distribution. Lesson 9.2 makes this connection more precise, by showing that minimising the (empirical) forward KLD is _the same thing_ as MLE!\n",
        "\n",
        "Awareness of how probabilistic models behave under misspecification can be very useful in understanding how machine-learning methods perform in real world, and for choosing the right method for a given job. We will return to discuss these different behaviours in module 10 on data generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b39ee375",
      "metadata": {
        "id": "b39ee375"
      },
      "source": [
        "## 9.2 Decision trees (9.75 Points)\n",
        "\n",
        "In this part of the exercise, you will be looking at implementing, training, and tuning decision trees. This process will make imporant use of concepts from information theory.\n",
        "\n",
        "For ease of visualisation, we will work on a simple dataset in two dimensions. The data that we will use here is a adapted from the classic Iris dataset first used by Fisher in 1936, which contains measurements of the flowers of three different kinds of Iris plants. The Iris data is very commonly used as an example dataset in machine learning and statistics.\n",
        "\n",
        "Our first step, as always, is to load the data into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elder-visibility",
      "metadata": {
        "id": "elder-visibility"
      },
      "outputs": [],
      "source": [
        "# Loading dataset\n",
        "iris = load_iris()\n",
        "columns=['sepal_length', 'sepal_width', 'label']\n",
        "dataset = pd.DataFrame(np.column_stack((iris.data[:, :2], iris.target)),\n",
        "                       columns=columns)\n",
        "dataset['label'] = dataset['label'].astype(np.int32)\n",
        "dataset_train, dataset_val = train_test_split(dataset, test_size=20, random_state=1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "innovative-affairs",
      "metadata": {
        "id": "innovative-affairs"
      },
      "source": [
        "Whenever you are confronted with a new dataset, it is usually a good idea to try to understand what is going on inside it. We have included code below that prints the dimensions of the data and plots a small visualisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0710d8c",
      "metadata": {
        "id": "d0710d8c"
      },
      "outputs": [],
      "source": [
        "print(f\"Train set: {dataset_train.shape}\\tTest set: {dataset_val.shape}\")\n",
        "\n",
        "# A small visualisation of the dataset\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.scatterplot(data=dataset_train, x=columns[0], y=columns[1], hue='label', ax=ax[0])\n",
        "ax[0].set_title('Train set')\n",
        "sns.scatterplot(data=dataset_val, x=columns[0], y=columns[1],ax=ax[1], hue='label')\n",
        "ax[1].set_title('Validation set')\n",
        "fig.suptitle('Visualising the dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "external-expression",
      "metadata": {
        "id": "external-expression"
      },
      "source": [
        "Looking at the information above we can see:\n",
        "* There are 130 points in the training data and 20 in the validation data\n",
        "* Each point is associated with three numbers; these turn out to be `sepal_length`, `sepal_width`, and a class `label`\n",
        "* There are three different classes, labelled 0 through 2\n",
        "* Class 0 can be separated out linearly using an oblique boundary, whereas classes 1 and 2 overlap in a more complicated way\n",
        "* The sepal measurements have been quantised to one decimal, and it is possible that points in the scatterplot may be overlapping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb44b523",
      "metadata": {
        "id": "fb44b523"
      },
      "source": [
        "### Decision-tree questions\n",
        "\n",
        "As described in the lecture notes, the idea of decision trees is to partition the observation space $\\Omega$ into decision regions by recursively splitting the space into smaller and smaller regions using axis-aligned splits. This can produce some quite complex-looking decision regions.\n",
        "\n",
        "For a numerical feature, like those in the current dataset, an axis-aligned split can be seen as a question of the form “is feature $x_d$ bigger than the threshold value $t$?” Depending on the answer to the question (here “yes” or “no”), we might ask different follow-up questions, and so on until a decision is reached. This creates a tree structure where internal nodes are questions, edges are answers to questions, and leaf nodes are decisions.\n",
        "\n",
        "Since each datapoint will lie in a decision region, and will have an answer to a question of the above type, we can view decision trees as not only partitioning the data space, but also as partitioning the dataset $\\mathcal D$. We have visualised the effect of splitting the data based on `sepal_length` for three different possible threshold values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31303c7b",
      "metadata": {
        "id": "31303c7b"
      },
      "outputs": [],
      "source": [
        "sns.lmplot(data=dataset_train, x=columns[0], y=columns[1], hue='label', fit_reg=False, aspect=1.5, legend=False)\n",
        "y_min, y_max = dataset_val.iloc[:, 1].min() - 1, dataset_val.iloc[:, 1].max() + 1\n",
        "plt.vlines(5, ymin=y_min, ymax=y_max, label=\"Q1: Is sepal_length > 5?\", colors='r')\n",
        "plt.vlines(6, ymin=y_min, ymax=y_max, label=\"Q2: Is sepal_length > 6?\", colors='k')\n",
        "plt.vlines(7, ymin=y_min, ymax=y_max, label=\"Q3: Is sepal_length > 7?\", color='b')\n",
        "plt.title('Examples of questions for a decision tree')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retired-chapel",
      "metadata": {
        "id": "retired-chapel"
      },
      "source": [
        "Building decision trees is about finding good questions to ask, that allow us to differentiate between the classes.\n",
        "\n",
        "**9.2.1 Question:**<br/>\n",
        "Does any of the three questions visualised (Q1, Q2, or Q3) above lead to a situation where we know what the class label of a datapoint is, if the answer to the question is “yes”? If so, which one of Q1, Q2, or Q3 is it? <font color=\"red\"> (0.25 point) </font>\n",
        "\n",
        "**9.2.2 Question:**<br/>\n",
        "Does any of the three questions visualised (Q1, Q2, or Q3) above lead to a situation where we know what the class label of a datapoint is, if the answer to the question is “no”? If so, which one of Q1, Q2, or Q3 is it? <font color=\"red\"> (0.25 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stunning-commitment",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fb3d2bfa50a9c993029475105986ad40",
          "grade": false,
          "grade_id": "cell-9f486b38f9cee798",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "stunning-commitment"
      },
      "outputs": [],
      "source": [
        "# Please uncomment the line that represents your answer to the first question\n",
        "#if_yes_then_we_know_the_answer_to = 'none' # If the answer is neither Q1, Q2, nor Q3\n",
        "#if_yes_then_we_know_the_answer_to = 'q1'\n",
        "#if_yes_then_we_know_the_answer_to = 'q2'\n",
        "#if_yes_then_we_know_the_answer_to = 'q3'\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# Please uncomment the line that represents your answer to the second question\n",
        "#if_no_then_we_know_the_answer_to = 'none' # If the answer is neither Q1, Q2, nor Q3\n",
        "#if_no_then_we_know_the_answer_to = 'q1'\n",
        "#if_no_then_we_know_the_answer_to = 'q2'\n",
        "#if_no_then_we_know_the_answer_to = 'q3'\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exterior-camel",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3599d1fd21fbed6a69577d4200f68025",
          "grade": true,
          "grade_id": "cell-e4979c3f92fff278",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "exterior-camel"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "assert if_yes_then_we_know_the_answer_to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e17143",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "afcaa73203a2ae25f2a6e58cb55d2e67",
          "grade": true,
          "grade_id": "cell-13aaade86214043b",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "86e17143"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "assert if_no_then_we_know_the_answer_to"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "original-processing",
      "metadata": {
        "id": "original-processing"
      },
      "source": [
        "Below is an implementation of Python class that represents a question that we can use to partition the dataset and (soon) build decision trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac4817c",
      "metadata": {
        "id": "0ac4817c"
      },
      "outputs": [],
      "source": [
        "class Question:\n",
        "    \"\"\"Ask a question to partition the dataset\"\"\"\n",
        "    columns = columns\n",
        "\n",
        "    def __init__(self, column, value):\n",
        "        assert column in [0, 1], \"A question can only be asked only on first and second column of the dataframe\"\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "\n",
        "    def answer(self, datapoints):\n",
        "        values = datapoints[self.column]\n",
        "        return values >= self.value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Is {Question.columns[self.column]} >= {self.value}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb204390",
      "metadata": {
        "id": "fb204390"
      },
      "source": [
        "The code below illustrates how this class can be used to create question objects and compute the answers to these questions for a datapoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c51494",
      "metadata": {
        "id": "e2c51494"
      },
      "outputs": [],
      "source": [
        "# Let's ask these four questions\n",
        "q1 = Question(0, 5)\n",
        "q2 = Question(0, 6)\n",
        "q3 = Question(0, 7)\n",
        "q4 = Question(1, 2)\n",
        "\n",
        "# These lines print the answer to the questions for the first point in the training data\n",
        "datapoint = dataset_train.values[0]\n",
        "print(f\"Datapoint: {list(zip(Question.columns, datapoint))[:-1]}\")\n",
        "print(f\"{q1}\\t{q1.answer(datapoint)}\")\n",
        "print(f\"{q2}\\t{q2.answer(datapoint)}\")\n",
        "print(f\"{q3}\\t{q3.answer(datapoint)}\")\n",
        "print(f\"{q4}\\t{q4.answer(datapoint)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc3fa98",
      "metadata": {
        "id": "3dc3fa98"
      },
      "source": [
        "**9.2.3 Task:**<br/>\n",
        "Create a variable `q` that expresses the question “Is `sepal_width >= 3`?” <font color=\"red\"> (0.25 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cb99ae",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "12bc3663ac8b4e4356c7e63d35194247",
          "grade": false,
          "grade_id": "cell-f3ae0cecf4e94be9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "a9cb99ae"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "print(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0b367c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2dc1ea4604ddac52b3f0a07e0610221b",
          "grade": true,
          "grade_id": "cell-b03231ceb42ea88c",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "dc0b367c"
      },
      "outputs": [],
      "source": [
        "assert q.answer(np.array([1.2, 5])), f\"Wrong question asked check the problem description and this '{q}'.\"\n",
        "assert not q.answer(np.array([7.3, 2])), f\"Wrong question asked check the problem description and this '{q}'.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cef3f2e0",
      "metadata": {
        "id": "cef3f2e0"
      },
      "source": [
        "### Implementing decision tree basics\n",
        "\n",
        "Throughout this exercise, we will gradually build an implementation of decision trees. Most of the code is already provided for you, but you will be asked to contribute some key pieces.\n",
        "\n",
        "First, we provide a number of pre-implemented basic functions and helper code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08d7061",
      "metadata": {
        "id": "f08d7061"
      },
      "outputs": [],
      "source": [
        "# These are helper functions. Please do not modify.\n",
        "\n",
        "class Leaf:\n",
        "    \"\"\"\n",
        "    A Leaf node used to classify the data\n",
        "\n",
        "    This holds a dictionary {1: x} where x is number of times\n",
        "    it appears in the rows from the training data that reach this leaf.\n",
        "    \"\"\"\n",
        "    def __init__(self, rows, depth):\n",
        "        self.predictions = dict(Counter(rows[:, -1]))\n",
        "        self.depth = depth\n",
        "        label = str(self.leaf_text())\n",
        "        self.visualize = pydot.Node(label+'-'+str(depth)+'-'+str(uuid4()), label=label, style=\"filled\", fillcolor=\"greenyellow\", shape='ellipse')\n",
        "\n",
        "    def leaf_text(self):\n",
        "        total = sum(self.predictions.values()) * 1.0\n",
        "        output = []\n",
        "        for lbl in self.predictions.keys():\n",
        "            output.append(f\"{int(lbl):d}={int(self.predictions[lbl] / total * 100):d}%\")\n",
        "        return \";\".join(output)\n",
        "\n",
        "\n",
        "class Decision_Node:\n",
        "    \"\"\"\n",
        "    Has a reference to the question, left sub tree and right sub tree.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 question,\n",
        "                 true_branch,\n",
        "                 false_branch,\n",
        "                 depth):\n",
        "        self.question = question\n",
        "        self.true_branch = true_branch\n",
        "        self.false_branch = false_branch\n",
        "        self.depth = depth\n",
        "        label = str(question)\n",
        "        self.visualize = pydot.Node(label+'-'+str(depth)+'-'+str(uuid4()), label=label, shape=\"rectangle\", fillcolor=\"azure2\", style=\"filled\")\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    Main decision tree class we will be adding more methods to it in the further parts of the assignement\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features=2, n_classes=3, max_depth=10, split_function=None):\n",
        "        super(DecisionTree, self).__init__()\n",
        "        assert max_depth > 0, '\"Be like a tree. Stay grounded, keep growing, and know when to let go.\" - Buddha. Also, the depth must be greater than 0.'\n",
        "        self.max_depth = max_depth\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = 3\n",
        "        self.tree = None\n",
        "        self.pydot_tree = pydot.Dot(\"Decision Tree\", label=\"Decision Tree\", graph_type=\"digraph\", strict=True, splines='line')\n",
        "        self.split_function = split_function\n",
        "\n",
        "    def fit(self, rows):\n",
        "\n",
        "        if not hasattr(self, 'build_tree'):\n",
        "            AttributeError(\"Method build_tree is not defined yet! Keep following the exercise to define it\")\n",
        "\n",
        "        if isinstance(rows, pd.DataFrame):\n",
        "            rows = rows.values\n",
        "\n",
        "        self.tree = self.build_tree(rows, 0)\n",
        "        self.generate_pydot_tree(self.tree, None, None)\n",
        "\n",
        "\n",
        "    def predict(self, rows):\n",
        "        if isinstance(rows, pd.DataFrame):\n",
        "            rows = rows.values\n",
        "\n",
        "        predictions = []\n",
        "        for row in rows:\n",
        "            key = list(self.predict_row(row, self.tree).keys())[0]\n",
        "            predictions.append(key)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "    def generate_pydot_tree(self, node, parent_node, edge_label):\n",
        "        self.pydot_tree.add_node(node.visualize)\n",
        "\n",
        "        if isinstance(node, Leaf):\n",
        "            self.pydot_tree.add_edge(pydot.Edge(parent_node.visualize, node.visualize, label=edge_label))\n",
        "            return\n",
        "\n",
        "        if parent_node is not None:\n",
        "            self.pydot_tree.add_edge(pydot.Edge(parent_node.visualize, node.visualize, label=edge_label))\n",
        "\n",
        "        self.generate_pydot_tree(node.true_branch, node, 'True')\n",
        "        self.generate_pydot_tree(node.false_branch, node, 'False')\n",
        "\n",
        "    def visualize_tree(self):\n",
        "        plt = Image(self.pydot_tree.create_png())\n",
        "        display(plt)\n",
        "\n",
        "    def visualize_decision_boundary(self, rows=None, plot_step=0.02, alpha=0.1, title=\"Decision boundary of the tree\"):\n",
        "\n",
        "        if rows is None:\n",
        "            raise ValueError(r\"Either pass 'rows=dataset_train' or 'rows=dataset_test' to \" +\n",
        "                       \"visualize the decision boundary and get insights on how the tree is fitting the data.\")\n",
        "\n",
        "        rows_df = rows\n",
        "        if isinstance(rows, pd.DataFrame):\n",
        "            rows = rows.values\n",
        "\n",
        "        delta = 0.25\n",
        "        x_min, x_max = rows[:, 0].min() - delta, rows[:, 0].max() + delta\n",
        "        y_min, y_max = rows[:, 1].min() - delta, rows[:, 1].max() + delta\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                             np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "        Z = self.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        cs = ax.contourf(xx, yy, Z, cmap='coolwarm_r', alpha=alpha)\n",
        "        plt.axis(\"tight\")\n",
        "\n",
        "        ax.set_title(title)\n",
        "        sns.scatterplot(data=rows_df, x=Question.columns[0], y=Question.columns[1], hue='label', ax=ax)\n",
        "        plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11caa4df",
      "metadata": {
        "id": "11caa4df"
      },
      "source": [
        "Next, we implement the part of the code that partitions a set of datapoints according to whether the answer to a given question is `True` or `False`.\n",
        "\n",
        "**9.2.4 Task:**<br/>\n",
        "Implement the `partitioning` function below. Aside from `self`, the function takes two inputs: <font color=\"red\"> (1 point) </font>\n",
        "* `rows`: An `np.array` of datapoints, one per row\n",
        "* `question`: A `Question` object, that will de used to split the data in `rows`\n",
        "The function should return a tuple of two outputs:\n",
        "* `true_rows`: All rows in `rows` for which the answer to `question` is `True`\n",
        "* `false_rows`: All rows in `rows` for which the answer to `question` is `False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b61f91",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b6d72a68d6103f2256693b7493a4931b",
          "grade": false,
          "grade_id": "cell-d73db1ec02fa80f3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "d9b61f91"
      },
      "outputs": [],
      "source": [
        "def partitioning(self, rows, question):\n",
        "    true_rows, false_rows = [], []\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return np.array(true_rows), np.array(false_rows)\n",
        "\n",
        "# Reinitialise the tree with the new method added\n",
        "DecisionTree.partitioning = partitioning\n",
        "decision_tree = DecisionTree()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f95fa86",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "06abca941ba34a68ff6b07681067fb1b",
          "grade": true,
          "grade_id": "cell-3ddd1a0e41404e50",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0f95fa86"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "greater_than_5, less_than_5 = partitioning(decision_tree, np.arange(10).reshape(5, 2), Question(1, 5))\n",
        "print(f\"Greater than 5: \\n{greater_than_5}\")\n",
        "print(f\"Less than 5: \\n{less_than_5}\")\n",
        "assert (greater_than_5.shape == (3, 2) and less_than_5.shape == (2, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "olympic-usage",
      "metadata": {
        "id": "olympic-usage"
      },
      "source": [
        "### Deciding which question to ask\n",
        "\n",
        "To decide which question to ask, decision trees evaluate which of the possible questions that most effectively separate the classes in the current set of datapoints. If a question perfectly separates all classes in the data (which only can happen if the datapoints belong to at most two classes) it is clearly a good chocie, but if doesn't perfectly separate the classes, we will need some sort of measure of how close a partitioning is to perfect separation.\n",
        "\n",
        "We will now look at two measures of how good a partitioning is:\n",
        "* Information gain\n",
        "* The Gini impurity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d705da61",
      "metadata": {
        "id": "d705da61"
      },
      "source": [
        "### Information gain\n",
        "\n",
        "The information gain is based on the classic Shannon entropy, computed over the distribution of class labels. We will need to implement that first.\n",
        "\n",
        "**9.2.5 Task:**<br/>\n",
        "Implement the `entropy` function for decision trees below. Note that each row in `rows` is a datapoint, and the label is the last element of the row. <font color=\"red\"> (1.5 point) </font>\n",
        "\n",
        "_Note:_<br/>\n",
        "This function needs to compute multiclass entropy. As a consequence, the entopy of the label distribution can exceed 1 bit, since we might need more than one bit to encode three different possibilities. Compare with the numbers 0 through 2 written in binary, `00`, `01`, `10`, which clearly need more than a single bit to be expressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbafc99",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "84cafb489704d4001a8d2361af5705ad",
          "grade": false,
          "grade_id": "cell-5236893a8b114b64",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8bbafc99"
      },
      "outputs": [],
      "source": [
        "def entropy(rows):\n",
        "    \"\"\"\n",
        "    Returns the entropy of the distribution of labels in rows.\n",
        "    Remember to handle the np.log2(0) case, for instance using np.where\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return(entropy)\n",
        "\n",
        "# Reinitialise the tree with the new method\n",
        "decision_tree = DecisionTree(split_function=entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4585c559",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0484d13d466abfa3c99537ac8608a801",
          "grade": true,
          "grade_id": "cell-0f6d11d9dd5dd99d",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4585c559"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "assert decision_tree.split_function(np.zeros((10, 1))) == 0.0, \"The entropy of a dataset where everything has the same class should be 0\"\n",
        "assert decision_tree.split_function(np.vstack((np.full((5, 1), 1), np.full((5, 1), 0)))) == 1.0, \"The entropy of an equal number of examples from two classes should be 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "general-affair",
      "metadata": {
        "id": "general-affair"
      },
      "source": [
        "A good question to ask in the decision tree is one that reduces the uncertainty regarding the label of an instance, i.e., one that reduces the entropy between the classes. We can see this idea of greedily trying to minimise the overall entropy as fast as possible as an instance of using the entropy as a loss function for the decision tree.\n",
        "\n",
        "A way to address this is to look at how much the entropy changes (decreases) after a particular split, compared to its value before the split. The prior entopy minus the new entropy is known as the _information gain_.\n",
        "\n",
        "We will now define information gain in a more mathematical manner: Let $\\{\\mathcal Y_1,\\mathcal Y_1\\}$ be a partition of the labels $\\mathcal Y =\\{\\mathcal y_i\\}_{i=1}^n$. Let $P(\\mathbf y \\in \\mathbf Y_2|\\mathrm y \\in \\mathcal Y)=p_1$ denote the probability that an element selected uniformly at random from $\\mathcal Y$ is a member of $\\mathcal Y_1$, and introduce the similarly probability $p_2=1-p_1$ for $\\mathcal Y_2$. If we define a random variable $\\mathbf s$ (for split) that has $P(\\mathbf s=1)=p_1$ and $P(\\mathbf s=2)=p_2$, we can write the information gain using an expected value over the entropies of the two partitions:\n",
        "$$\\mathrm{IG}(\\mathcal Y_1,\\mathcal Y_2) = H(\\mathcal Y) - \\mathbb E_{P_{\\mathbf s}}[H(\\mathcal Y_{\\mathbf s})]$$\n",
        "In a sense, this is just the difference in expected (loss) before and after the particitioning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9d4858",
      "metadata": {
        "id": "0c9d4858"
      },
      "source": [
        "We can expand the expression for information gain into a more concrete and easy-to-implement formula as\n",
        "$$\\mathrm{IG}(\\mathcal Y_1,\\mathcal Y_2) = H(\\mathcal Y) - p_1 \\cdot H(\\mathcal Y_1) -  p_2 \\cdot H(\\mathcal Y_2)\\\\\n",
        "= H(\\mathcal Y_1 \\cup \\mathcal Y_2) - \\frac{|\\mathcal Y_1|}{|\\mathcal Y_1 \\cup \\mathcal Y_2|} H(\\mathcal Y_1) - \\frac{|\\mathcal Y_2|}{|\\mathcal Y_1 \\cup \\mathcal Y_2|} H(\\mathcal Y_2)$$\n",
        "where $|\\mathcal Y|$ is the total number of elements in $\\mathcal Y$.\n",
        "\n",
        "**9.2.6 Task:**<br/>\n",
        "Implement the `improvement` function below, which measures how much the loss function improves from a given partitioning. Aside from `self` the inputs are:\n",
        "* `left`: A partition of datapoints (a.k.a. rows) corresponding to $\\mathcal Y_1$.\n",
        "* `right`: A partition of datapoints (a.k.a. rows) corresponding to $\\mathcal Y_2$\n",
        "* `current_uncertainty`: The entropy $H(\\mathcal Y)$ of (the distribution of the labels in) the union of the partitions.\n",
        "\n",
        "The function returns a single number, namely the information gain $\\mathrm{IG}(\\mathcal Y_1,\\mathcal Y_2)$ of the partitioning. <font color=\"red\"> (1 point) </font>\n",
        "\n",
        "_Important:_<br/>\n",
        "You can access the `entropy` function that you implemented above as `self.split_function`. Please use the latter, instead of calling `entropy` directly, since it makes the code generalise to other splitting functions that we will consider later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0413a8ed",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c15cfa204cecf12793d30058750feb38",
          "grade": false,
          "grade_id": "cell-1d664d619d72fe2a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0413a8ed"
      },
      "outputs": [],
      "source": [
        "def improvement(self, left, right, current_uncertainty):\n",
        "    \"\"\"\n",
        "    Return the improvement to the loss function gained from the left-right partitioning.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Reinitialise the tree with the new method added\n",
        "DecisionTree.improvement = improvement\n",
        "decision_tree = DecisionTree(split_function=entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0165890b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2733880ea395fb54a36dd56a670cab2c",
          "grade": true,
          "grade_id": "cell-55819c9963253f8d",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0165890b"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "left, right = np.vstack((np.full((3, 1), 0), np.full((3, 1), 1))), np.full((5, 1), 1)\n",
        "assert np.isclose(decision_tree.improvement(left, right, 0.5), -0.045, atol=1e-2), \"Wrong value of improvement please check the implementation again\"\n",
        "left, right = np.vstack((np.full((10, 1), 0), np.full((3, 1), 1))), np.full((15, 1), 1)\n",
        "assert np.isclose(decision_tree.improvement(left, right, 0.3), -0.061, atol=1e-2), \"Wrong value of improvement please check the implementation again\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de65919",
      "metadata": {
        "id": "0de65919"
      },
      "source": [
        "### Gini impurity\n",
        "\n",
        "Another common measure of how good a decision tree is the _Gini impurity_ of a leaf node (i.e., a set of datapoints). This should not be confused with the Gini _coefficient_ from economics; they're just named after the same person.\n",
        "\n",
        "The Gini impurity can be seen as the answer to the question “If randomly guess labels according to the a priori distribution on this dataset, what misclassification rate will I get?” If we let $\\mathbf y$ be a label drawn uniformly at random from $\\mathcal Y$, then we have\n",
        "$$P(\\mathbf y=\\mathrm y)=\\pi_{\\mathrm y}=\\frac{n_{\\mathrm y}}{n}$$\n",
        "where $n_{\\mathrm y}$ is the number of instances in $\\mathcal Y$ that equal $\\mathrm y$. We can then write\n",
        "$$ G(\\mathcal Y)\n",
        "= \\sum_{\\mathrm y=1}^k \\pi_{\\mathrm y} (1 - \\pi_{\\mathrm y})\n",
        "= \\mathbb E_{P_{\\mathbf y}}[(1-P(\\mathbf y=\\mathrm y))]$$\n",
        "\n",
        "If we look at the above expression, it is quite similar to the expression for the (Shannon) entropy\n",
        "$$ H(\\mathcal Y)\n",
        "= \\sum_{\\mathrm y=1}^k \\pi_{\\mathrm y} (-\\log_2\\pi_{\\mathrm y})\n",
        "= \\mathbb E_{P_{\\mathbf y}}[(-\\log_2 P(\\mathbf y=\\mathrm y))]$$\n",
        "The Gini impurity just replaces $-\\log_2\\pi_{\\mathrm y}$ in this expectation with $1 - \\pi_{\\mathrm y}$. Both of these are decreasing functions that reach zero when $\\pi_{\\mathrm y}=1$.\n",
        "_We can view the Gini impurity as another measure of entropy, i.e., uncertainty._\n",
        "\n",
        "For a Bernoulli random variable, the Gini impurity corresponds to replacing the function for the entropy of a single coin toss, which is almost-but-not-quite a parabola, with an actual parabola. Another name for this function is the _Tsallis entropy of index 2_. This quantity does not have all of the appealing theoretical properties of the classic Shannon entropy, but it is quite close, and it might even work better in certain scenarios, since the Tsallis entopy is less sensitive to changes to the probability of rare events, compared to the Shannon entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hispanic-pillow",
      "metadata": {
        "id": "hispanic-pillow"
      },
      "source": [
        "To compute the Gini impurity in practice, we can simplify the previous expression slightly\n",
        "$$ G(\\mathcal Y)\n",
        "= \\sum_{\\mathrm y=1}^k \\pi_{\\mathrm y} (1 - \\pi_{\\mathrm y})\n",
        "= \\sum_{\\mathrm y=1}^k \\pi_{\\mathrm y} - \\sum_{\\mathrm y=1}^k\\pi_{\\mathrm y}^2\n",
        "= 1 - \\sum_{\\mathrm y=1}^k\\pi_{\\mathrm y}^2$$\n",
        "\n",
        "**9.2.7 Task:**<br/>\n",
        "Implement the `gini` function for decision trees below. Note that each row in `rows` is a datapoint, and the label is the last element of the row. <font color=\"red\"> (1 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07846868",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9fe07349d80544f60efd2ac9365b66d",
          "grade": false,
          "grade_id": "cell-ccec919ce81f3cbf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "07846868"
      },
      "outputs": [],
      "source": [
        "def gini(rows):\n",
        "    \"\"\"Return the Gini impurity for a collection of datapoints.\"\"\"\n",
        "    counts = dict(Counter(rows[:, -1]))\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return impurity\n",
        "\n",
        "# Reinitialise the tree with the new method\n",
        "decision_tree = DecisionTree(split_function=gini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01702964",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aedf98d3a325979aea846db1d75a8899",
          "grade": true,
          "grade_id": "cell-23181122f5f6d0c3",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "01702964"
      },
      "outputs": [],
      "source": [
        "# Run pre-specified tests (do not edit)\n",
        "assert decision_tree.split_function(np.zeros((10, 1))) == 0.0, \"The Gini impurity of a dataset where everything has the same class should be 0\"\n",
        "assert decision_tree.split_function(np.vstack((np.full((5, 1), 1), np.full((5, 1), 0)))) == 0.5, \"The Gini impurity of an equal number of examples from two classes should be 0.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dcd6db",
      "metadata": {
        "id": "42dcd6db"
      },
      "source": [
        "We can now use this function instead of $H(\\mathcal Y)$ as a loss function for building decision trees, and try to find the spilts that reduce the expected value of $G(\\mathcal Y)$.\n",
        "\n",
        "As a small example, how much does the Gini impurity decrease if using our previous question `q` using the loss `func=gini`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacdb4b5",
      "metadata": {
        "id": "cacdb4b5"
      },
      "outputs": [],
      "source": [
        "current_uncertainity = gini(dataset_train.values)\n",
        "print(f\"Current uncertainity:\\t{current_uncertainity: .3f}\")\n",
        "petal_width_above, petal_width_below = partitioning(decision_tree, dataset_train.values, q)\n",
        "information_gain = improvement(decision_tree, petal_width_above, petal_width_below, current_uncertainity)\n",
        "print(f\"Loss reduction:\\t{information_gain: .3f} with the question '{q}'\")\n",
        "#print(f\"After splitting left tree contains the classes {np.unique(petal_width_below[:, -1])} \" +\n",
        "#      f\"and the right tree contains the classes {np.unique(petal_width_above[:, -1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d84d24",
      "metadata": {
        "id": "88d84d24"
      },
      "source": [
        "### Building the tree\n",
        "\n",
        "What we need to do next on the roadmap toward decision trees is to consider all relevant splits, all relevant questions we can ask, and try to see which one of them that will decrease the loss the most. Normally, this would be done by binning every continuous-valued feature and then comparing the potential loss improvements from splitting the data along each bin boundary. However, since our dataset is already quantised to one decimal, it has already effectively been binned, and we can just iterate over all unique feature values in the data.\n",
        "\n",
        "**9.2.8 Task:**<br>\n",
        "Try to find the best split by iterating over all features (columns) and all rows (examples) in the dataset by implementing `find_best_split` below. For each feature, consider all possible threshold values that we can use when asking a question that splits the data based on that feature. Return the size of the best improvement that you found and the question that gave rise to it. <font color=\"red\"> (1.5 point) </font>\n",
        "\n",
        "_Hint:_\n",
        "Use `self.partitioning` to split the rows according to a specific question and then use `self.improvement` to compute how much the loss improves (here called `gain`) due to the resulting split. Splits where either $\\mathcal Y_1$ or $\\mathcal Y_2$ end up empty can be ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ea4dd1",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "708bedf9d7685533a8259304d8e1bc85",
          "grade": false,
          "grade_id": "cell-b9e241b877c8154c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "37ea4dd1"
      },
      "outputs": [],
      "source": [
        "def find_best_split(self, rows):\n",
        "    best_gain = 0\n",
        "    best_question = None\n",
        "    current_uncertainty = self.split_function(rows)\n",
        "\n",
        "    for col in range(self.n_features):  # for each feature\n",
        "        values = set([row[col] for row in rows])  # unique values in the column\n",
        "        for val in values:  # for each value\n",
        "            question = Question(col, val)\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    return best_gain, best_question\n",
        "\n",
        "# Reinitialise the tree with the new method added\n",
        "DecisionTree.find_best_split = find_best_split\n",
        "decision_tree = DecisionTree(split_function=gini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb046d0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c77eb2b61a163927c1530f7c5a1fdf2d",
          "grade": true,
          "grade_id": "cell-2fd7a9b4aa46cb33",
          "locked": true,
          "points": 1.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3fb046d0"
      },
      "outputs": [],
      "source": [
        "dummy_data = np.array([\n",
        "    [1, 2, 0],\n",
        "    [2, 3, 0],\n",
        "    [4, 5, 1],\n",
        "    [4, 3, 1]\n",
        "])\n",
        "\n",
        "best_gain, best_question = find_best_split(decision_tree, dummy_data)\n",
        "assert best_gain == 0.5, \"The best gain does not seems to be correct\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a213017",
      "metadata": {
        "id": "1a213017"
      },
      "source": [
        "We can now put together code that leverages the various functions we already have created, in order to recursively split the data to build a decision tree. For your convenience, this has already been done for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18cec037",
      "metadata": {
        "id": "18cec037"
      },
      "outputs": [],
      "source": [
        "def build_tree(self, rows, depth):\n",
        "    \"\"\"\n",
        "    A recursive, depth-first way to build the tree.\n",
        "\n",
        "    Rules of recursion:\n",
        "        1) Believe that it works.\n",
        "        2) Start by checking for the base case (no further information gain or max depth reached).\n",
        "        3) Prepare for giant stack traces.\n",
        "    \"\"\"\n",
        "    # Try partitioing the dataset on each of the unique attribute,\n",
        "    # calculate the information gain,\n",
        "    # and return the question that produces the highest gain.\n",
        "    gain, question = self.find_best_split(rows)\n",
        "\n",
        "    # Base case: no further improvement or max depth reached\n",
        "    # Since we can ask no further questions,\n",
        "    # we'll return a leaf.\n",
        "    if gain == 0 or depth == self.max_depth:\n",
        "        return Leaf(rows, depth)\n",
        "\n",
        "    # If we reach this point, we have found a useful feature / value\n",
        "    # to partition on.\n",
        "    true_rows, false_rows = self.partitioning(rows, question)\n",
        "\n",
        "    # Recursively build the true branch.\n",
        "    true_branch = self.build_tree(true_rows, depth + 1)\n",
        "\n",
        "    # Recursively build the false branch.\n",
        "    false_branch = self.build_tree(false_rows, depth + 1)\n",
        "\n",
        "    # Return a Question node.\n",
        "    # This records the best feature / value to ask at this point\n",
        "    # as well as the branches to follow\n",
        "    # depending on the answer.\n",
        "    return Decision_Node(question, true_branch, false_branch, depth)\n",
        "\n",
        "\n",
        "# Reinitialise the tree with the new method added\n",
        "DecisionTree.build_tree = build_tree\n",
        "decision_tree = DecisionTree(split_function=entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "planned-plasma",
      "metadata": {
        "id": "planned-plasma"
      },
      "source": [
        "Let's use the code we have to build a tree on our training dataset (using `decision_tree.fit`). After that, we can visualise the tree we got using `decision_tree.visualize_tree()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ceae54a",
      "metadata": {
        "id": "7ceae54a"
      },
      "outputs": [],
      "source": [
        "decision_tree.fit(dataset_train)\n",
        "decision_tree.visualize_tree()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93be72d",
      "metadata": {
        "id": "a93be72d"
      },
      "source": [
        "We still need to implement a function that takes new instances and predicts their class label. That function is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cf1d4e",
      "metadata": {
        "id": "c9cf1d4e"
      },
      "outputs": [],
      "source": [
        "def predict_row(self, row, node=None):\n",
        "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
        "    if node is None:\n",
        "        node = self.tree\n",
        "\n",
        "    # Base case: we've reached a leaf\n",
        "    if isinstance(node, Leaf):\n",
        "        return node.predictions\n",
        "\n",
        "    # Decide whether to follow the true-branch or the false-branch.\n",
        "    # Compare the feature / value stored in the node,\n",
        "    # to the example we're considering.\n",
        "    if node.question.answer(row):\n",
        "        return self.predict_row(row, node.true_branch)\n",
        "    else:\n",
        "        return self.predict_row(row, node.false_branch)\n",
        "\n",
        "\n",
        "# Reinitialise the tree with the new method added\n",
        "DecisionTree.predict_row = predict_row\n",
        "decision_tree = DecisionTree(split_function=gini)\n",
        "decision_tree.fit(dataset_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ddd77e0",
      "metadata": {
        "id": "1ddd77e0"
      },
      "source": [
        "Let's see what predictions we get on the validation data, and check what macro $F_1$ score those predictions have. We can use the `f1_score` function (with suitable arguments) to compute the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dcafdcf",
      "metadata": {
        "id": "1dcafdcf"
      },
      "outputs": [],
      "source": [
        "prediction = decision_tree.predict(dataset_val)\n",
        "f1_score(prediction, dataset_val.values[:, -1], average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e15ab9f",
      "metadata": {
        "id": "8e15ab9f"
      },
      "source": [
        "That's not perfect, but it's definitely above random chance (if not, there's a mistake somewhere).\n",
        "\n",
        "Let's use `visualize_decision_boundary` to see what the decision regions for the three classes look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1840b00",
      "metadata": {
        "id": "b1840b00"
      },
      "outputs": [],
      "source": [
        "decision_tree.visualize_decision_boundary(dataset_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef8eeca1",
      "metadata": {
        "id": "ef8eeca1"
      },
      "source": [
        "Hmm. That looks a wee bit idiosyncratic.\n",
        "\n",
        "Can we do better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e15bde",
      "metadata": {
        "id": "e9e15bde"
      },
      "source": [
        "### Regularise the decision tree\n",
        "\n",
        "The decision tree procedure we have been using thus far does not stop until it is nearly\n",
        "100% accurate on training data. it should be pretty obvious that this can lead to overfitting.\n",
        "\n",
        "There exists a large variety of different techniques that can be used to regularise decision-tree learning, with the goal of getting less overfitted trees that generalise better. Here we will be using a variant of early stopping, where we constrain the maximum depth of the tree. This means that we don't go on to recursively build subtrees for the child nodes if we already have reached a certain, maximum depth.\n",
        "\n",
        "Below is code that runs our decision-tree learning with the maximum depth constrained to 5. In the visualisation of the resulting tree, note that not all branches go all the way to the maximum depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed961ce",
      "metadata": {
        "id": "5ed961ce"
      },
      "outputs": [],
      "source": [
        "decision_tree = DecisionTree(split_function=entropy, max_depth=5)\n",
        "decision_tree.fit(dataset_train)\n",
        "prediction = decision_tree.predict(dataset_val)\n",
        "decision_tree.visualize_tree()\n",
        "#accuracy_score(prediction, dataset_test.values[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32eda932",
      "metadata": {
        "id": "32eda932"
      },
      "source": [
        "What is the best choice of loss function and maximum depth, in terms of validation-set performance? That is for you to find out!\n",
        "\n",
        "**9.2.9 Task:**<br/>\n",
        "Do a hyperparameter search over all possible values of `max_depth` from 1 to 10, and for each of the two loss functions you implemented earlier. For every tree you build this way, print the maximum depth that you used and the loss function that you get, along with the resulting macro $F_1$ scores on the training and validation sets. <font color=\"red\"> (1.5 point) </font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "Do things in this order:\n",
        "1. Fit the data to the training set: `.fit(dataset_train)`\n",
        "2. Compute predictions on the training set: `.predict(dataset_train)`\n",
        "3. Compute the macro f1 score with: `sklearn.measure.f1_score(***, average='macro')`\n",
        "4. Compute predictions on the validation set: `.predict(dataset_val)`\n",
        "5. Compute the macro f1 score with: `sklearn.measure.f1_score(***, average='macro')`\n",
        "6. Print the values for both the training and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ff07ee",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3c66f5b0ec44ceb26fa489090d41a8f0",
          "grade": true,
          "grade_id": "cell-b5ea4a73a8ea0caa",
          "locked": false,
          "points": 1.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "84ff07ee"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc2af39",
      "metadata": {
        "id": "fdc2af39"
      },
      "source": [
        "**9.2.10 Question:**<br/>\n",
        "Which combination of maximum depth and loss function achieved the best performance on the validation set? <font color=\"red\"> (0.25 point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e44990",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6868cbf025a2376abc706345339170f5",
          "grade": true,
          "grade_id": "cell-1945720d841f95f9",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "79e44990"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c6c230",
      "metadata": {
        "id": "e2c6c230"
      },
      "source": [
        "**9.2.11 Question:**<br/>\n",
        "What performance (in terms of validation-set macro $F_1$ score) did the hyperparameters from the previous question give? <font color=\"red\"> (0.25 point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0ea9f6",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c99f7adacc77ac47283ce26a9bacd322",
          "grade": true,
          "grade_id": "cell-5e1c2900fb2e1ef8",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1f0ea9f6"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f7ca81",
      "metadata": {
        "id": "04f7ca81"
      },
      "source": [
        "**9.2.12 Task:**<br/>\n",
        "Visualise the tree with the best validation-set macro $F_1$ performance. <font color=\"red\"> (0.5 point) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8af01b5",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "380a5e77cc4509c30ffe27d8eaa6b2ac",
          "grade": true,
          "grade_id": "cell-65dafcd136143905",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c8af01b5"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7846d8b5",
      "metadata": {
        "id": "7846d8b5"
      },
      "source": [
        "**9.2.13 Task:**<br/>\n",
        "As a final comparison, visualise the boundaries of two different trees: One with the optimal depth you found, and another trained with maximum depth 10. <font color=\"red\"> (0.5 point) </font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "You can train new models in the cell. To show the decision regions, use `visualize_decision_boundary` as shown earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187695d3",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "386411c24a37d73f5677dcaac2165db7",
          "grade": true,
          "grade_id": "cell-8c17b242adc552f4",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "187695d3"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9af8300",
      "metadata": {
        "id": "e9af8300"
      },
      "source": [
        "This should have given you a pretty decent idea of how decision trees work and what kind of decision regions they create. :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "23fbe0f7",
        "fb44b523",
        "cef3f2e0",
        "olympic-usage",
        "d705da61",
        "0de65919",
        "88d84d24",
        "e9e15bde"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}