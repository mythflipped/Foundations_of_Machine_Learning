{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6ff748bb",
      "metadata": {
        "id": "6ff748bb"
      },
      "source": [
        "# General Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a122dbf",
      "metadata": {
        "id": "0a122dbf"
      },
      "source": [
        "Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the [Code of Conduct](https://kth.instructure.com/courses/32018/pages/code-of-conduct). In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions.\n",
        "\n",
        "If you need assistance with the exercise, you are encouraged to post a question to the appropriate [Discussion Topic](https://kth.instructure.com/courses/32018/discussion_topics) or sign up for a help session.\n",
        "\n",
        "<br>\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and delete the `raise NotImplementedError()` once you have implemented the solution\n",
        "\n",
        "<br>\n",
        "\n",
        "You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all` then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows\n",
        "\n",
        "<br>\n",
        "\n",
        "`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`\n",
        "\n",
        "<br>\n",
        "\n",
        "where you replace `??` with the correct exercise number. If you are working alone you do not need to include a partner name. Correctly naming the file and including your name (and your partner's) below is worth **1 point** - if you fail to correctly name the file or include your partner's name, *you will lose this point*.\n",
        "\n",
        "<br>\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9308e84b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4e10dd9ff9316230e586bc2ddf7c902a",
          "grade": false,
          "grade_id": "cell-d2aae5414ee22e91",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "9308e84b"
      },
      "source": [
        "# Name (1 pts)\n",
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly): <font color=\"red\"> (1 Point) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c98f13",
      "metadata": {
        "id": "23c98f13"
      },
      "source": [
        "\n",
        "### Student: LAST, FIRST\n",
        "\n",
        "### Partner: LAST, FIRST"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b23ae7f0",
      "metadata": {
        "id": "b23ae7f0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9477de7e",
      "metadata": {
        "id": "9477de7e"
      },
      "source": [
        "# Exercise 9: Data generation (9 pts)\n",
        "\n",
        "In this exercise, you will train a few very simple models for data generation, and see some of the basic principles of data generation in action.\n",
        "\n",
        "_Note:_<br/>\n",
        "The methods you will be using in this exercise are far from being state of the art. They are only intended as teaching tool, and to avoid the time required to train a deep generative model. You can take it as a fun challenge to come up with new ideas for how to improve the approaches you consider work on here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff83aa0",
      "metadata": {
        "id": "7ff83aa0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044fe3b1",
      "metadata": {
        "id": "044fe3b1"
      },
      "outputs": [],
      "source": [
        "SEED=1234\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d164bb",
      "metadata": {
        "id": "c8d164bb"
      },
      "source": [
        "## 9.1 Loading and visualising the dataset (0.5 pts total)\n",
        "\n",
        "We begin by loading the data for the exericse. The data is a subset of the Labeled Faces in the Wild database (Huang et al., 2007), containing monochrome photographs of human faces. Each image is labelled to identify different photographs of the same person.\n",
        "\n",
        "For simplicity, we only load the data from the 500 persons with the most images in the data. These are usually various celebrities. You might recognise a few of them.\n",
        "\n",
        "_Note:_<br/>\n",
        "Loading the data might take some time during the first run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5dafb53",
      "metadata": {
        "id": "d5dafb53"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "    X, y = fetch_lfw_people(return_X_y=True)\n",
        "    X = X * 255\n",
        "    X = X.round()\n",
        "    # Reducing size of the dataset with the 500 most frequent labels\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    labels_to_keep = np.argpartition(counts, -500)[-500:]\n",
        "    X_ = []\n",
        "    for label in labels_to_keep:\n",
        "        indexes = (y == label)\n",
        "        X_.append(X[indexes])\n",
        "    X = np.concatenate(X_)\n",
        "    np.random.shuffle(X)\n",
        "    return X\n",
        "\n",
        "X = load_dataset()\n",
        "image_shape = (62, 47)\n",
        "\n",
        "print(np.shape(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db89d81b",
      "metadata": {
        "id": "db89d81b"
      },
      "source": [
        "The images are 62 pixels tall and 47 pixels wide, for a total of 2914 pixels (2914 elements in the feature vector for each observation).\n",
        "\n",
        "Let's visualise a few training examples, randomly selected from the training database, to see what they look like.\n",
        "\n",
        "_Hint:_<br/>\n",
        "Take note of what the code looks like, since you will have to plot the data yourself later. In particular, we use `np.reshape` to reshape the vector to a matrix and `plotting_axis.imshow` to actuall plot the images in matplotlib. We also pass `cmap='gray', vmin=0, vmax=255` as parameters to `imshow`, otherwise the faces will all look like the Hulk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5267b0",
      "metadata": {
        "id": "ff5267b0"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(10, 6))\n",
        "for i in range(10):\n",
        "    plotting_axis = ax[i//5][i % 5]\n",
        "    plotting_axis.imshow(X[i].reshape(*image_shape), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15ab30a",
      "metadata": {
        "id": "f15ab30a"
      },
      "source": [
        "Let's also visualise the average of all images in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c23734",
      "metadata": {
        "id": "f5c23734"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "ax.imshow(X.mean(0).reshape(image_shape), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe42f8cb",
      "metadata": {
        "id": "fe42f8cb"
      },
      "source": [
        "**Question 9.1.1:**<br/>\n",
        "Name one or two ways that the above average image visually differs from real faces in the databases. <font color=\"red\"> (0.5 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb804be",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9308968e83879422de5506704a917086",
          "grade": true,
          "grade_id": "cell-793e534e907eb870",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "beb804be"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c7cf80c",
      "metadata": {
        "id": "8c7cf80c"
      },
      "source": [
        "This average image is actually the “face” that minimises the mean squared error when compared to all the images in the training database.\n",
        "\n",
        "Obviously, a deterministic model like this is not a good fit for unconditional face synthesis. This is not unexpected, since there is a lot of variation in the database, as seen from the examples we plotted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3f8771",
      "metadata": {
        "id": "8a3f8771"
      },
      "source": [
        "## 9.2 A simple, Gaussian model (4 pts total)\n",
        "\n",
        "In this part, we will fit a simple generative probabilistic model to the data. In fact, we will use a Gaussian distribution, since that's the first distribution that basically everyone who ever studied probability tends to think of.\n",
        "\n",
        "_Remark:_<br/>\n",
        "Again, this model is too simple to make sense in practice, but it is interesting to see what happens. Also, it is generally a better idea to start from a simple model and then add complexity, rather than the other way around."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9ef0e4",
      "metadata": {
        "id": "5f9ef0e4"
      },
      "source": [
        "Can we even describe this data using a Gaussian distribution? Let's take a look at the numerical values in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5c8d52",
      "metadata": {
        "id": "4d5c8d52"
      },
      "outputs": [],
      "source": [
        "print(X[0])\n",
        "print(X[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c98a111",
      "metadata": {
        "id": "0c98a111"
      },
      "source": [
        "**Task 9.2.1**:<br/>\n",
        "Print the biggest and smallest value in the entire dataset. <font color=\"red\"> (0.25 pts) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98555f9b",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b585038bb94c53ba032390c25a07a57d",
          "grade": true,
          "grade_id": "cell-0da47530d7da2bed",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "98555f9b"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7e692e",
      "metadata": {
        "id": "7f7e692e"
      },
      "source": [
        "It seems like the values are discretised to the integers $\\{0, 1, \\ldots, 255\\}$, i.e., 8 bits of precision. This is very common with image data.\n",
        "\n",
        "**Question 9.2.2**:<br/>\n",
        "Motivate why it is not considered appropriate to model such data using a continuous distribution like a Gaussian. <font color=\"red\"> (0.5 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d375c039",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e322d7b388d110d735909d274432d469",
          "grade": true,
          "grade_id": "cell-c53aa9ed6c9a2b4e",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "d375c039"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf59704",
      "metadata": {
        "id": "6bf59704"
      },
      "source": [
        "### Dequantisation\n",
        "\n",
        "Many image models (perhaps even most of them) treat images as continuous data. For this to work they apply something known as _dequantisation_, and we will do the same here.\n",
        "\n",
        "**Task 9.2.3**:<br/>\n",
        "Add independent uniform noise on $[0,1)$ to every element in the dataset and store the result in `X_deq`. We have added code to print the values from a few example images after dequantisation. <font color=\"red\"> (0.5 pts) </font>\n",
        "\n",
        "_Note:_<br/>\n",
        "Dequantisation is not strictly necessary in this exercise because the models we use are so simple, but it's considered best practise for image data, so we recommend that you always do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20573845",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7907c32216d62ad1bc51882127905d1b",
          "grade": true,
          "grade_id": "cell-a9247aacb8d0b5a1",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "20573845"
      },
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "print(X_deq[0])\n",
        "print(X_deq[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a1b20c",
      "metadata": {
        "id": "63a1b20c"
      },
      "source": [
        "Note that the new values only differ from the values printed earlier in their decimal part. (If not, you might have made a mistake, so go back and check your approach again.)\n",
        "\n",
        "If everything looks good, we will now overwrite `X` with the new `X_deq`, to use the dequantised data from now on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "567e9d4c",
      "metadata": {
        "id": "567e9d4c"
      },
      "outputs": [],
      "source": [
        "X = X_deq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfae6528",
      "metadata": {
        "id": "cfae6528"
      },
      "source": [
        "_Remark:_<br/>\n",
        "Even though dequantisation is likely not to have much of an effect on the results in this exercise, it can be very important when working with deep generative models. It should therefore be a part of every data-processing pipeline when using continuous distributions to model image data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0525268",
      "metadata": {
        "id": "d0525268"
      },
      "source": [
        "### Parameter estimation\n",
        "\n",
        "Next, we will fit a Gaussian to the dequantised dataset.\n",
        "\n",
        "First, let us define $\\mathcal X$ as the set of all training images, and let $\\mathcal X_j$ be the values for pixel (element) in this training data. The former contains vectors $x_i\\in\\mathbb R^{hw}$, the latter contains scalar values $x_{ij}\\in\\mathbb R$ (for $j\\in\\{1,\\ldots,hw\\}$).\n",
        "\n",
        "**Task 9.2.4:**<br/>\n",
        "Assume that each pixel is Gaussian distributed and independent of all other pixels. Show that the MLE $\\widehat\\theta_{\\mathrm{ML}}(\\mathcal X)$ in this case can be written in terms of the MLE of $\\theta_j=(\\mu_j,\\sigma_j)$ computed from each pixel’s dataset $\\mathcal X_j$. <font color=\"red\"> (1 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60337ea5",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "02b6953ee8b2b92d8305617031e7de1c",
          "grade": true,
          "grade_id": "cell-ae2d389bdfe62b4e",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "60337ea5"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3d42a23",
      "metadata": {
        "id": "a3d42a23"
      },
      "source": [
        "**Task 9.2.5**:<br/>\n",
        "Use the above result together with the standard formulas for maximum likelihood estimation of scalar Gaussian distributions, to compute the MLE parameter estimate $\\widehat\\theta_{\\mathrm{ML}}(\\mathcal X)$ for this model, where each pixel is independently Gaussian distributed. <font color=\"red\"> (0.5 pts) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc85a763",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b68a187d17546fb6906a1cdfd2fd32f1",
          "grade": true,
          "grade_id": "cell-6516a6fdd132b33c",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cc85a763"
      },
      "outputs": [],
      "source": [
        "X_j_mu = None # Replace with code that computes the mean vector of the multivariate Gaussian\n",
        "X_j_std = None # Replace with code that computes the vector of standard deviations for the multivariate Gaussian\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4510bd28",
      "metadata": {
        "id": "4510bd28"
      },
      "source": [
        "### Generating output\n",
        "\n",
        "Now let's see what we get if we sample output from this distribution.\n",
        "\n",
        "**Task 9.2.6**:<br/>\n",
        "Use the command `np.random.randn()` to implement the function `sample_from_normal` below, which samples an instance from the fitted Gaussian distribution. `np.random.randn()` generates a matrix of independent samples from the (scalar) standard normal distribution. <font color=\"red\"> (1 pts) </font>\n",
        "\n",
        "_Note:_<br/>\n",
        "Your implementation must use `np.random.randn()`.\n",
        "\n",
        "_Hint:_<br/>\n",
        "You need to change the location and scale of the random samples in order to transform them from $\\mathcal{N}(0, 1)$ to samples from a Gaussian with the required mean and standard deviation, $x_j\\sim\\mathcal{N}(\\mu_j, \\sigma_j)$. This is the basis of something known as the “reparametrisation trick” in variational autoencoders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65fd51bf",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "461bf3259c5c286ecf777d25a8d2dce1",
          "grade": false,
          "grade_id": "cell-1d0cdbe2c1de943b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "65fd51bf"
      },
      "outputs": [],
      "source": [
        "def sample_from_normal():\n",
        "    global X_j_mu\n",
        "    global X_j_std\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eccbb90",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "85a43a8e0224f04b64438111ba3de193",
          "grade": true,
          "grade_id": "cell-e564b1024af481a0",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3eccbb90"
      },
      "outputs": [],
      "source": [
        "# Please do not edit these testcases\n",
        "np.random.seed(SEED)\n",
        "test_sample = sample_from_normal()\n",
        "assert np.isclose(test_sample.mean(), 127.23515, atol=1e-4), \"Did you set the location and scaling of the Gaussian correctly?\"\n",
        "assert np.isclose(test_sample.std(), 46.74192, atol=1e-4), \"Invalid implementation, please check the scaling again.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c22da36",
      "metadata": {
        "id": "2c22da36"
      },
      "source": [
        "Let's visualise some random samples from this Gaussian model of facial images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6779479b",
      "metadata": {
        "id": "6779479b"
      },
      "outputs": [],
      "source": [
        "samples = [sample_from_normal() for _ in range(5)]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(samples[i].clip(0, 256).reshape(image_shape), cmap='gray', vmin=0, vmax=255)\n",
        "ax[2].set_title(\"Images sampled from learnt distribution\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5129c109",
      "metadata": {
        "id": "5129c109"
      },
      "source": [
        "**Question 9.2.7:**<br/>\n",
        "Are the images better or worse than the mean image you visualised earlier? Describe the main problem with the images. <font color=\"red\"> (0.25 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffcf4ce",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a7a5e698802906c6450982947e8fe4d1",
          "grade": true,
          "grade_id": "cell-c3abc2dfa7dde830",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6ffcf4ce"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf2d050",
      "metadata": {
        "id": "7bf2d050"
      },
      "source": [
        "## 9.3 Improving the approach (4.5 pts total)\n",
        "\n",
        "We will now look at how we can improve our image model.\n",
        "\n",
        "You might have noticed a lot of noise in your sampled images, a bit like a snowstorm. The problem here is that we assumed that, for each pixel, the difference from the mean value was independent of every other pixel. That is not true in practice. If a pixel is very bright (for example, it is part of someone's teeth in a gleaming smile) it is very likely that nearby pixels also are teeth, and also are brighter than the average. This simple reasoning shows that our model is much too simplistic for this data, since it ignores correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf78adbe",
      "metadata": {
        "id": "cf78adbe"
      },
      "source": [
        "### Representing the data in a way that better fits our model\n",
        "\n",
        "How can we get better images? It seems like one straightforward idea would be to include correlations between pixels into the model, by fitting a multivariate Gaussian with a full (instead of diagonal) covariance matrix to the pixel data. However, this would involve estimating a covariance matrix with $2914^2\\approx 8,500,000$ elements, which is really big.\n",
        "\n",
        "Instead of adding correlations to the model, another idea is to _remove_ correlations from the _data_. For uncorrelated data, a model that ignores correlations should do a lot better. You might recall from the geometry module that principal component analysis, PCA, turns correlated data into uncorrelated features. More specifically, the principal components are orthogonal vectors, and the PCA are coefficients mutually uncorrelated. These coefficients can therefore be described well by a simple Gaussian with diagonal covariance. Crucially, principal components can be “inverted”, in the sense that we can recreate the data/images from the PCA coefficients. Thus, a model that generates PCA coefficients also generates face images.\n",
        "\n",
        "As a bonus, we can do dimensionality reduction, since most of what is going on in the data (most of the variation) is described by the first components from the PCA, and we can reconstruct close approximations of all images from their leading PCA coefficients.\n",
        "\n",
        "We have arrived at the following overall approach:\n",
        "1. Start from image data\n",
        "2. Apply PCA with $K$ components to the data\n",
        "3. Each image is now described by $K$ uncorrelated numbers (coefficients), that describe how much of each component we need to take to reconstruct that specific image from the components\n",
        "4. Fit a multivariate Gaussian with diagonal covariance to the PCA coefficient data\n",
        "5. Sample new coefficients from the model\n",
        "6. Reconstruct images from the randomly sampled coefficients using the inverse PCA\n",
        "\n",
        "Because the components in the PCA affect multiple pixels, the pixels in the sampled output images are now correlated, and should look less noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e1483eb",
      "metadata": {
        "id": "1e1483eb"
      },
      "source": [
        "Let's now take the first step and apply PCA to the data. We will take $K=250$ components, which should capture nearly all of the variation in the dataset.\n",
        "\n",
        "_Important:_<br/>\n",
        "We use an underscore after variable and function names `_` to denote PCA. For example, `X_` contains the coefficients that describe the images in PCA space. The variable `pca` contains a representation of the PCA, including the principal components themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc5f4aa",
      "metadata": {
        "id": "0cc5f4aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "n_components = 250\n",
        "\n",
        "pca = PCA(n_components, random_state=SEED)\n",
        "X_ = pca.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0afeb95",
      "metadata": {
        "id": "e0afeb95"
      },
      "source": [
        "Let's visualise the principal axes (components) in image space. The images we get describe the directions of maximum variance in the data, in descending order. The resulting images are often called _eigenfaces_, since they are computed using a kind of eigenvector analysis under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f23c5d",
      "metadata": {
        "id": "85f23c5d"
      },
      "outputs": [],
      "source": [
        "eigenfaces = pca.components_[:n_components]\n",
        "\n",
        "# Show the first 16 eigenfaces\n",
        "fig, axes = plt.subplots(4,4,sharex=True,sharey=True,figsize=(8,10))\n",
        "for i in range(16):\n",
        "    axes[i%4][i//4].imshow(eigenfaces[i].reshape(image_shape), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b815ae",
      "metadata": {
        "id": "b0b815ae"
      },
      "source": [
        "We can see that the eigenface images are blurry, since they have been averaged across many different faces. Nonetheless, we can tell that all of them are related to faces, and they can be combined in different ways to build pictures of different faces.\n",
        "\n",
        "With a little imagination, we can give an interpretation regarding what kind of variation that some of these components describe.\n",
        "\n",
        "**Question 9.3.1:**<br/>\n",
        "Which component (image) above would you expect to show up a lot in images of faces wearing sunglasses? Give your answer as a number, where 1, 2, 3, 4 are the images in the first row (from left to right), 5, 6, 7, 8 are the images in the second row (again from left to right), and so on. <font color=\"red\"> (0.25 pts) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00cfceb6",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8603a7aa35314a1b07026e8aa4699050",
          "grade": false,
          "grade_id": "cell-9763198323265e7d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "00cfceb6"
      },
      "outputs": [],
      "source": [
        "sunglasses_image_number = 0\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a3f11b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "38ddd06b3c13deb556f3c6722778ba3f",
          "grade": true,
          "grade_id": "cell-4ba8071d94d148c8",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "92a3f11b"
      },
      "outputs": [],
      "source": [
        "assert sunglasses_image_number > 0\n",
        "assert sunglasses_image_number <= 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3beaae0b",
      "metadata": {
        "id": "3beaae0b"
      },
      "source": [
        "**Task 9.3.2**:<br/>\n",
        "Fit a new model to the PCA data $\\mathcal X_{\\mathrm{PCA}}$, assuming that each coefficient is independently Gaussian distributed. This gives the MLE parameter estimate $\\widehat\\theta_{\\mathrm{ML}}(\\mathcal X_{\\mathrm{PCA}})$. It's the same approach and formulas that you used earlier, just applied to a different set of numbers (PCA coefficients instead of pixels). <font color=\"red\"> (0.25 pts) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c565aeb",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d1969808c7a0e0c310557f09b29de553",
          "grade": true,
          "grade_id": "cell-6215dbd0de396849",
          "locked": false,
          "points": 0.25,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5c565aeb"
      },
      "outputs": [],
      "source": [
        "X_j_mu_ = None # Replace with code that computes the mean of the multivariate Gaussian on the PCA data\n",
        "X_j_std_ = None # Replace with code that computes the vector of standard deviations for the multivariate Gaussian on the PCA data\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b282d5",
      "metadata": {
        "id": "c7b282d5"
      },
      "source": [
        "Now let's see what images we get if we sample output from this new model.\n",
        "\n",
        "**Task 9.3.3**:<br/>\n",
        "Implement the function `sample_from_normal_` below, which is identical to `sample_from_normal` from before, except that we sample an instance from the Gaussian distribution that we fitted to the PCA coefficients instead. <font color=\"red\"> (0.25 pts) </font>\n",
        "\n",
        "_Note:_<br/>\n",
        "Your function should not convert the coefficients back into images. We will do that further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf923c50",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b07a3643cb54fa15b99519ff1187962c",
          "grade": false,
          "grade_id": "cell-356cf343ec069901",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bf923c50"
      },
      "outputs": [],
      "source": [
        "def sample_from_normal_():\n",
        "    global X_j_mu_\n",
        "    global X_j_std_\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25a720e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c307e1506fe2fdd5cc958402c95cb457",
          "grade": true,
          "grade_id": "cell-5227159ba89c5aaa",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "d25a720e"
      },
      "outputs": [],
      "source": [
        "# Please do not edit these testcases\n",
        "np.random.seed(SEED)\n",
        "test_sample = sample_from_normal_()\n",
        "assert np.isclose(test_sample.mean(), 4.15971, atol=1e-4), \"Did you set the location and scaling of the Gaussian correctly?\"\n",
        "assert np.isclose(test_sample.std(), 128.34241, atol=1e-4), \"Invalid implementation, please check the scaling again.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "097e3236",
      "metadata": {
        "id": "097e3236"
      },
      "source": [
        "Let's visualise some images produced by random samples from this new Gaussian model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84da32a6",
      "metadata": {
        "id": "84da32a6"
      },
      "outputs": [],
      "source": [
        "samples_ = [sample_from_normal_() for _ in range(5)]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from the learnt distribution of principal components\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a54c2e",
      "metadata": {
        "id": "99a54c2e"
      },
      "source": [
        "Hooray! The random “faces” might look pretty scary, but at least all that noise is gone, like we wanted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfee9889",
      "metadata": {
        "id": "dfee9889"
      },
      "source": [
        "### Reducing the temperature\n",
        "\n",
        "A common trick to get more appealing output from generative models is to reduce the amount of randomness in the output. Let's reduce the entropy of the fitted distribution, to concentrate it on the most probable outcomes, and see what happens.\n",
        "\n",
        "**Task 9.3.4:**<br/>\n",
        "Re-implement the `sample_from_normal_` function above to take an argument `temperature` on $[0, 1]$ that scales the standard deviation proportionally. This is known as “reducing the temperature”. <font color=\"red\"> (0.25 pts) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcf894d0",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "19f438a9315469290a1204e1175b690e",
          "grade": false,
          "grade_id": "cell-941cdd438e2e3cc1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dcf894d0"
      },
      "outputs": [],
      "source": [
        "def sample_from_normal_(temperature=1):\n",
        "    global X_j_mu_\n",
        "    global X_j_std_\n",
        "    assert 0 <= temperature <= 1, \"The temperature parameter must be between 0 and 1.\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d1530e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "af21b768614ef4f65232d3e7e817e44a",
          "grade": true,
          "grade_id": "cell-c3f9feab01e2bef2",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "c7d1530e"
      },
      "outputs": [],
      "source": [
        "# Please do not edit these testcases\n",
        "np.random.seed(SEED)\n",
        "test_sample_ = sample_from_normal_(temperature=1)\n",
        "assert np.isclose(test_sample.mean(), 4.15971, atol=1e-4), \"Did you set the location and scaling of the Gaussian correctly?\"\n",
        "assert np.isclose(test_sample.std(), 128.34241, atol=1e-4), \"Invalid implementation, please check the scaling again.\"\n",
        "\n",
        "test_sample_ = sample_from_normal_(temperature=0.5)\n",
        "assert np.isclose(test_sample_.mean(), 1.20034, atol=1e-4), \"The temperature has been used incorrectly. Check the implementation\"\n",
        "assert np.isclose(test_sample_.std(), 91.24741, atol=1e-4), \"The temperature has been used incorrectly. Check the implementation\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714cb9bf",
      "metadata": {
        "id": "714cb9bf"
      },
      "source": [
        "Below are plots of output images sampled with different settings for the temperature parameter, namely 1, 0.25, and 0. The latter will give the maximally probable output under our model, which is the same very time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd4e5a9",
      "metadata": {
        "id": "3fd4e5a9"
      },
      "outputs": [],
      "source": [
        "samples_ = pca.inverse_transform([sample_from_normal_(1) for _ in range(5)])\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(samples_[i].reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from learnt distribution (T=1)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_ = pca.inverse_transform([sample_from_normal_(0.25) for _ in range(5)])\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(samples_[i].reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from learnt distribution (T=0.25)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_ = pca.inverse_transform([sample_from_normal_(0) for _ in range(5)])\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(samples_[i].reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from learnt distribution (T=0)\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8cf089",
      "metadata": {
        "id": "3c8cf089"
      },
      "source": [
        "**Question 9.3.5**:<br/>\n",
        "Did reducing the temperature improve the perceived quality? Give one or two sentences of motivation that describe what got better (or worse). <font color=\"red\"> (0.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636afea4",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "db6480f6a5e6fcbcad900141401b0d37",
          "grade": true,
          "grade_id": "cell-8f6a1cccab3ea8bc",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "636afea4"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5773feff",
      "metadata": {
        "id": "5773feff"
      },
      "source": [
        "**Question 9.3.6**:<br/>\n",
        "The images on the last row should look familiar. Where have you seen them before? Explain what happens when the temperature is 0. <font color=\"red\"> (0.5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0275c585",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6e8548521d28da9dcb2d382ae3912020",
          "grade": true,
          "grade_id": "cell-69c3958128d30a90",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0275c585"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bedf06e",
      "metadata": {
        "id": "0bedf06e"
      },
      "source": [
        "Despite the improvements from performing PCA, it is clear that the model is very far from perfect."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c35d97a9",
      "metadata": {
        "id": "c35d97a9"
      },
      "source": [
        "### Making the model non-Gaussian\n",
        "\n",
        "Thus far we have only considered Gaussian models of the data. Since PCA is just a linear (or affine) transformation of the data, our model in PCA space also corresponds to a Gaussian model of images, just with a better description of how the pixels covary. In reality, however, the data is not Gaussian, and that allows for strong dependencies between the values of different PCA coefficients, even if the coefficients are uncorrelated.\n",
        "\n",
        "To see that the image data isn't Gaussian, we will study eight different pixels (elements) on the primary diagonal of the original image data (no PCA). We plot a histogram for each pixel, showing the distribution of the brightness values of that pixel across the image data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c85d4b",
      "metadata": {
        "id": "17c85d4b"
      },
      "outputs": [],
      "source": [
        "diagonal = [(i, i) for i in range(image_shape[1])][0:30:4]\n",
        "print(f\"Primary diagonal pixels: {diagonal}\")\n",
        "\n",
        "fig, ax = plt.subplots(1, len(diagonal), figsize=(20,5))\n",
        "\n",
        "for i in range(len(diagonal)):\n",
        "    ax[i].hist(X[:, len(diagonal) * i + i])\n",
        "    ax[i].set_title(len(diagonal) * i + i)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd9013a",
      "metadata": {
        "id": "dfd9013a"
      },
      "source": [
        "**Question 9.3.7**:<br/>\n",
        "How can you tell from the histograms that these distributions are not Gaussian? Give one or two sentences of motivation. <font color=\"red\"> (0.5 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871e5373",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7deda462c4ab49c33843b74491983f27",
          "grade": true,
          "grade_id": "cell-904957be7dcd9a85",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "871e5373"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac72537",
      "metadata": {
        "id": "cac72537"
      },
      "source": [
        "To make progress, let's try to model the PCA data using a non-Gaussian distribution, namely a Gaussian mixture model.\n",
        "\n",
        "In addition to being able to describe non-Gaussian distributions, a GMM also has the advantage that it can capture the correlations\n",
        "\n",
        "Importantly, a GMM can in principle capture between pixels to an extent, via the mean vectors of different components. This is true even if each component has a diagonal covariance matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7821cdab",
      "metadata": {
        "id": "7821cdab"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c84d31",
      "metadata": {
        "id": "07c84d31"
      },
      "source": [
        "**Task 9.3.8**:<br/>\n",
        "Train a number of GMMs with different number of components, on the PCA data `X_`. At the very least, train one model (`gmm25_`) with 25 components, and another (`gmm250_`) with 250 components. <font color=\"red\"> (1 pts) </font>\n",
        "\n",
        "_Hint:_<br/>\n",
        "Use the scikit-learn implementation of GMMs (`GaussianMixture`), as imported above, and set `covariance_type='diag'` to use diagnonal covariance matrices for each component.\n",
        "\n",
        "_Note:_<br/>\n",
        "Because of the high dimentionality and the large dataset it might take some time to train these GMMs. However, training should be stable, since the scikit-learn implementation contains a variation of variance flooring. That said, there might still be a `ConvergenceWarning` or two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "793554e9",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2d9fcb37b32ba8be21912265d95f6012",
          "grade": true,
          "grade_id": "cell-3494beaa600fdd7a",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "793554e9"
      },
      "outputs": [],
      "source": [
        "gmm25_ = None\n",
        "gmm250_ = None\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fffefb6",
      "metadata": {
        "id": "3fffefb6"
      },
      "source": [
        "Let's take a look at some samples from these models and compare them to the previous Gaussian model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db6d79f",
      "metadata": {
        "id": "2db6d79f"
      },
      "outputs": [],
      "source": [
        "samples_ = [sample_from_normal_(1) for _ in range(5)]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 1 component\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_, _ = gmm25_.sample(5)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 25 components\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_, _ = gmm250_.sample(5)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 250 components\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1740fea7",
      "metadata": {
        "id": "1740fea7"
      },
      "source": [
        "The faces are still not the greatest, so we can probably benefit from reducing the temperature when sampling.\n",
        "\n",
        "The procedure that we used for a single Gaussian earlier is based on the temperature in the Gibbs measure, a concept that pops up in advanced probability theory and statistical physics, for example. We can't solve analytically for the distribution that results from any given Gibbs-measure temperature-reduction in a general Gaussian mixture distribution, but we can approximate the noise-reducing effect of temperature reduction by proportionally shrinking the covariance matrices of the different components. The code below implements that approximation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa531bac",
      "metadata": {
        "id": "fa531bac"
      },
      "outputs": [],
      "source": [
        "def samples_with_temperature(gmm, temperature=1, n_samples=5):\n",
        "    \"\"\" Sklearn does not has a method/argument to do it so here is a hacky way of doing it\"\"\"\n",
        "\n",
        "    assert 0 <= temperature <= 1, \"The temperature parameter must be between 0 and 1.\"\n",
        "\n",
        "\n",
        "    original_cov = gmm.covariances_.copy() # copying the original covariances\n",
        "    gmm.covariances_ = gmm.covariances_ * temperature # Adjusting temperature\n",
        "    samples, _ = gmm.sample(n_samples) # Sample\n",
        "    gmm.covariances_ = original_cov # reset the temperature\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f330880",
      "metadata": {
        "id": "8f330880"
      },
      "source": [
        "Let's plot some samples with reduced temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272aa580",
      "metadata": {
        "id": "272aa580"
      },
      "outputs": [],
      "source": [
        "samples_ = [sample_from_normal_(0.25) for _ in range(5)]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 1 component (T=0.25)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_ = samples_with_temperature(gmm25_, 0.25, 5)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 25 components (approx. T=0.25)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_ = samples_with_temperature(gmm250_, 0.25, 5)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 250 components (approx. T=0.25)\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bcd859f",
      "metadata": {
        "id": "8bcd859f"
      },
      "outputs": [],
      "source": [
        "samples_ = [sample_from_normal_(0) for _ in range(5)]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 1 component (T=0)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_ = samples_with_temperature(gmm25_, 0, 5)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 25 components (approx. T=0)\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "samples_ = samples_with_temperature(gmm250_, 0, 5)\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(10, 6))\n",
        "for i in range(5):\n",
        "    plotting_axis = ax[i]\n",
        "    plotting_axis.imshow(pca.inverse_transform(samples_[i]).reshape(image_shape), cmap='gray')\n",
        "ax[2].set_title(\"Images sampled from trained GMM with 250 components (approx. T=0)\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7040961",
      "metadata": {
        "id": "b7040961"
      },
      "source": [
        "**Question 9.3.9**:<br/>\n",
        "Why do the samples from GMMs with many components look less blurred? Explain in one or two sentences. <font color=\"red\"> (0.5 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c9c1ee6",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8cf7b3b528b18bef5791116d29be14d2",
          "grade": true,
          "grade_id": "cell-3aef0c17dee53a87",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7c9c1ee6"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8bf692",
      "metadata": {
        "id": "1d8bf692"
      },
      "source": [
        "**Question 9.3.10**:<br/>\n",
        "Why do the samples from many-component GMMs don't look exactly the same every time, even when we set the approximate temperature to 0? Explain in one or two sentences. <font color=\"red\"> (0.5 pts) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5040223f",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9f3c147ead1ec8fc8c03030dd94ea1ce",
          "grade": true,
          "grade_id": "cell-026f901363dc6616",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5040223f"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d423e9f",
      "metadata": {
        "id": "4d423e9f"
      },
      "source": [
        "Although the models you have trained in this exercise are not very strong image-generation methods, we hope that you nontheless have gotten an impression of some of the basic principles of data generation, and what we can do to iteratively build better models and obtain better output."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}